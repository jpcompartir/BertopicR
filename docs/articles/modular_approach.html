<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Interacting with Individual Modules • BertopicR</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Interacting with Individual Modules">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">BertopicR</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/modular_approach.html">Interacting with Individual Modules</a></li>
    <li><a class="dropdown-item" href="../articles/manipulating-the-model.html">Manipulating the Model</a></li>
    <li><a class="dropdown-item" href="../articles/quick_start.html">Topic Modelling without Optimisation</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/jpcompartir/BertopicR" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Interacting with Individual Modules</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/jpcompartir/BertopicR/vignettes/modular_approach.Rmd" class="external-link"><code>vignettes/modular_approach.Rmd</code></a></small>
      <div class="d-none name"><code>modular_approach.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://jpcompartir.github.io/BertopicR/" class="external-link">BertopicR</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://dplyr.tidyverse.org" class="external-link">dplyr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span><span class="st">"TOKENIZERS_PARALLELISM"</span> <span class="op">=</span> <span class="st">"0"</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="modules">Modules<a class="anchor" aria-label="anchor" href="#modules"></a>
</h2>
<p>In the <a href="https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html" class="external-link">Bertopic
Python Library</a> by Maarten Grootendorst there are 6 sub-modules:</p>
<ol style="list-style-type: decimal">
<li><p><span style="color:#4472C4">Embeddings</span> - for transforming
your documents into a numerical representation</p></li>
<li><p><span style="color:#4472C4">Dimensionality Reduction</span> - for
reducing the number of features of the embeddings output</p></li>
<li><p><span style="color:#ED7D31">Clustering</span> - finding groups of
similar documents to represent as topics</p></li>
<li><p><span style="color:#FFC000">Vectorisers</span> - find the n-grams
to describe each topic</p></li>
<li><p><span style="color:#FFC000">c-TF-IDF</span> - create topic-level
(rather than document) bag of words matrices for representing
topics</p></li>
<li><p>Fine-tuning topic representations - other tools for topic
representations (includes generative AI/LLMs), you can read more about
this in the <a href="manipulating-the-model.Rmd">Manipulating the
Model</a> vignette.</p></li>
</ol>
<p>This vignette will show you how to use {BertopicR} to tune each
module when creating your topic models.</p>
<div class="section level3">
<h3 id="data">Data<a class="anchor" aria-label="anchor" href="#data"></a>
</h3>
<p>For demonstrative purposes, we’ll use {stringr}‘s ’sentences’ data
set, which comes fairly clean. For help on cleaning text data visit
ParseR/LimpiaR documentation. Let’s take a look at the first five posts
for brevity.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sentences</span> <span class="op">&lt;-</span> <span class="fu">stringr</span><span class="fu">::</span><span class="va"><a href="https://stringr.tidyverse.org/reference/stringr-data.html" class="external-link">sentences</a></span></span>
<span><span class="va">sentences</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span></code></pre></div>
<p>Then we’ll turn the sentences into a data frame:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>sentences <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="va">sentences</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="embeddings">Embeddings<a class="anchor" aria-label="anchor" href="#embeddings"></a>
</h3>
<p>In order to work efficiently with text data, we need to turn the
words into numbers. The current state-of-the-art approach to turning
text into numbers, is contextualised word embeddings. We’ll use an MpNet
model, “all-minilm-l6-v2” to take our sentences and turn them into
numbers (embeddings). This will allow us to find similarities and
differences between our sentences, using standard mathematical
techniques (don’t worry if this isn’t making sense right now, often the
best way to learn is by doing).</p>
<blockquote>
<p>We use “all-minilm-l6-v2” for practical reasons - it is small and
powerful. This makes the process of developing BertopicR less painful.
You will most likely want to explore more powerful models like
“all-mpnet-base-v2”.</p>
</blockquote>
<p>In BertopicR you will usually either want to be making or doing with
modules, or, compiling or fitting with models. We make components, we do
actions on data with components. We compile our components into models,
and then we fit our models to data.</p>
<div class="section level4">
<h4 id="make-the-embedder">Make the embedder<a class="anchor" aria-label="anchor" href="#make-the-embedder"></a>
</h4>
<p>First we’ll make an embedder (or embedding_model) using the
<code>bt_make_embedder_st</code> function. Then we’ll embed our
sentences using the embedder.</p>
<p>TIP: It’s a good idea to save your embeddings, as when working with
many documents this process will be time consuming.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embedder</span> <span class="op">&lt;-</span> <span class="fu">BertopicR</span><span class="fu">::</span><span class="fu"><a href="../reference/bt_make_embedder_st.html">bt_make_embedder_st</a></span><span class="op">(</span></span>
<span>  model <span class="op">=</span> <span class="st">"all-minilm-l6-v2"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-embedding">Do the embedding<a class="anchor" aria-label="anchor" href="#do-the-embedding"></a>
</h4>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_embedding.html">bt_do_embedding</a></span><span class="op">(</span></span>
<span>  <span class="va">embedder</span>,</span>
<span>  <span class="va">df</span><span class="op">$</span><span class="va">sentences</span>,</span>
<span>  accelerator <span class="op">=</span> <span class="cn">NULL</span></span>
<span>  <span class="op">)</span></span>
<span></span>
<span><span class="va">embeddings</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span></code></pre></div>
<p>Each row of our embeddings output represents one of our original
sentences, and each column represents a different embedding dimension;
there are 384 dimensions outputted by the “all-minilm-l6-v2” model</p>
<p>We take a peek at the first 10 columns (dimensions), of the first row
of our embeddings and we see 10 floating point numbers.</p>
</div>
</div>
<div class="section level3">
<h3 id="reducing-dimensions">Reducing Dimensions<a class="anchor" aria-label="anchor" href="#reducing-dimensions"></a>
</h3>
<p>The next step in the pipeline is to reduce the dimensions of our
embeddings, we do this for two reasons:</p>
<ol style="list-style-type: decimal">
<li><p>to allow our clustering algorithm to run smoothly</p></li>
<li><p>to visualise our clusters on a plane (we will eventually reduce
to 2 dimensions)</p></li>
</ol>
<p>In machine learning more generally, dimensionality reduction is often
an important step to avoid overfitting and the curse of dimensionality.
In this example we will use the UMAP algorithm (however dimensionality
reduction using PCA and truncatedSVD are also currently available) for
more information on the UMAP algorithm - uniform manifold approximation
and projection for dimension reduction - <em>catchy</em>, see <a href="https://umap-learn.readthedocs.io/en/latest/" class="external-link">UMAP Docs</a>.</p>
<p>TIP: Like with embeddings, it’s a good idea to save your reduced
embeddings, as reducing dimensions can be a costly process.</p>
<div class="section level4">
<h4 id="make-the-reducer">Make the reducer<a class="anchor" aria-label="anchor" href="#make-the-reducer"></a>
</h4>
<p>We’ll use a low-ish value for n_neighbours (we have a small dataset)
and an output with 5 dimensions (n_components = 5L). We’ll set the
min_distance to 0, so that our dimensionality reduction model can place
very similar documents <em>very</em> close together. We’ll set the
metric to “Euclidean” see <a href="https://umap-learn.readthedocs.io/en/latest/embedding_space.html" class="external-link">Embedding
to non-Euclidean Spaces</a> for alternatives.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reducer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_reducer_umap.html">bt_make_reducer_umap</a></span><span class="op">(</span></span>
<span>  n_neighbours <span class="op">=</span> <span class="fl">10L</span>,</span>
<span>  n_components <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>  min_dist <span class="op">=</span> <span class="fl">0L</span>,</span>
<span>  metric <span class="op">=</span> <span class="st">"euclidean"</span></span>
<span>  <span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-reducing">Do the reducing<a class="anchor" aria-label="anchor" href="#do-the-reducing"></a>
</h4>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reduced_embeddings</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_reducing.html">bt_do_reducing</a></span><span class="op">(</span></span>
<span>  <span class="va">reducer</span>, embeddings <span class="op">=</span> <span class="va">embeddings</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">reduced_embeddings</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="op">]</span></span></code></pre></div>
<p>We’ll take a peek at two of our rows which now represent the reduced
dimension embeddings for each document. Notice that our numbers are
floating points, but also that they are not bounded between -1 and
1.</p>
<p>The next step is to cluster our data. On a first pass, bertopic
considers each discovered cluster a topic. Choice of clustering model
and the selected parameters are therefore important. We’ll use an
hdbscan cluster, as that’s what bertopic was initially built with.</p>
</div>
</div>
<div class="section level3">
<h3 id="clustering">Clustering<a class="anchor" aria-label="anchor" href="#clustering"></a>
</h3>
<p>There is a lot to learn when it comes to clustering, and selecting
the correct parameters is notoriously difficult - especially when
clustering without pre-assigned labels, as <em>most</em> clustering
tends to be. For this run we’ll use the hdbscan clustering algorithm,
because we don’t know how many clusters we should look for in advance
(which we should if using kMeans clustering for example).<a href="https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html" class="external-link">hdbscan
documentation</a></p>
<p>It’s important to know that until you get down to the level of
updating topic representations, in the bertopic pipeline 1 topic = 1
cluster. It’s therefore crucial to gather what information you can about
your data to inform your clustering process.</p>
<div class="section level4">
<h4 id="make-the-clusterer">Make the clusterer<a class="anchor" aria-label="anchor" href="#make-the-clusterer"></a>
</h4>
<p>We’ll stick with a Euclidean distance metric, we’ll reduce the
min_cluster_size to 10, giving us a theoretical maximum of number of
clusters as: length(sentences) / 10 and min_samples equal to 5. The
relationship between min_cluster_size and min_samples is important, it
will default to min_samples = min_cluster_size if not specified, but
this is likely to have adverse effects on your clustering outputs when
dealing with larger datasets (as you’ll likely want to raise the
min_cluster_size parameter significantly). On the other hand, the
hdbscan documentation claims that min_samples, a parameter inherited
from dbscan, does not have such importance in the hdbscan algorithm -
though they also say it remains the algorithm’s biggest weakness.</p>
<p>We’ll also set cluster_selection_method = “leaf”, this means we’ll
tend to find many small clusters, rather than a few large clusters. This
is another parameter which is fraught with danger, to get this right the
first time is unlikely, and is likely to require trial and error, at
least in the beginning.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clusterer</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_clusterer_hdbscan.html">bt_make_clusterer_hdbscan</a></span><span class="op">(</span>min_cluster_size <span class="op">=</span> <span class="fl">5L</span>,</span>
<span>                          metric <span class="op">=</span> <span class="st">"euclidean"</span>,</span>
<span>                          cluster_selection_method <span class="op">=</span> <span class="st">"leaf"</span>,</span>
<span>                          min_samples <span class="op">=</span> <span class="fl">3L</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="do-the-clustering">Do the clustering<a class="anchor" aria-label="anchor" href="#do-the-clustering"></a>
</h4>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">clusters</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_do_clustering.html">bt_do_clustering</a></span><span class="op">(</span>clustering_model <span class="op">=</span> <span class="va">clusterer</span>, embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span><span class="op">)</span></span></code></pre></div>
<p>clusters is now a list of cluster labels, we have 720 labels in total
- one for each sentence in {Stringr}’s sentences data set. The cluster
labels are output as integers, but it’s important <strong>not</strong>
to assume that they work like regular integers do. It’s <strong>not
necessarily</strong> the case that cluster 1 is closer to cluster 4 than
it is to cluster 15, the ordering of the labels can effectively be
considered random.</p>
<p>As you most likely have no labels or a training/test/validation data
set, you will have to rely on inspecting your clusters, remembering that
in bertopic 1 cluster = 1 topic. To check whether our clusters make
sense, we could draw upon our data analysis &amp; visualisation tool
kit, and inspect each cluster against every other. This would soon
become intractable. Instead, we’ll take a quick look at the distribution
and then we’ll use the bt_compile_model() and bt_fit_model() functions
to get our topic models out.</p>
</div>
<div class="section level4">
<h4 id="create-a-data-frame">Create a data frame<a class="anchor" aria-label="anchor" href="#create-a-data-frame"></a>
</h4>
<p>But first, we’re beginning to acquire a bunch of objects which may
become hard to maintain. We can store them in a data frame:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op">&lt;-</span> <span class="fu">dplyr</span><span class="fu">::</span><span class="fu"><a href="https://tibble.tidyverse.org/reference/tibble.html" class="external-link">tibble</a></span><span class="op">(</span>sentence <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/chartr.html" class="external-link">tolower</a></span><span class="op">(</span><span class="va">sentences</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>embeddings <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">embeddings</span><span class="op">)</span>,</span>
<span>         reduced_embeddings <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">reduced_embeddings</span><span class="op">)</span>,</span>
<span>         cluster <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">as.integer</a></span><span class="op">(</span><span class="va">clusters</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>If you want to save this data frame, you’ll need to save it as a
.Rdata/.rds object, not as a .csv or .xlsx as it contains list
columns.</p>
</div>
<div class="section level4">
<h4 id="count-the-clusters">Count the clusters<a class="anchor" aria-label="anchor" href="#count-the-clusters"></a>
</h4>
<p>We can see the distribution via a histogram:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">!=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">cluster</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x<span class="op">=</span> <span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html" class="external-link">geom_histogram</a></span><span class="op">(</span>fill <span class="op">=</span> <span class="st">"midnightblue"</span>, bins <span class="op">=</span> <span class="fl">20</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html" class="external-link">theme_minimal</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">xlab</a></span><span class="op">(</span><span class="st">"Cluster Size"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html" class="external-link">ylab</a></span><span class="op">(</span><span class="st">"Number of Clusters"</span><span class="op">)</span></span></code></pre></div>
<p>With the exception of the outlier group, the clusters are labelled in
order of size:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/count.html" class="external-link">count</a></span><span class="op">(</span><span class="va">cluster</span>, sort <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p>213/720 (29.6%) of data points were labelled as noise (cluster ==
-1), and upon a first inspection they do appear to be quite
eclectic.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/slice.html" class="external-link">slice</a></span><span class="op">(</span><span class="fl">30</span><span class="op">:</span><span class="fl">40</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>But how do our other clusters look?</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">0</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>The more clusters we look at, the more difficult it will become to
figure out what’s happening…</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html" class="external-link">filter</a></span><span class="op">(</span><span class="va">cluster</span> <span class="op">==</span> <span class="fl">2</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html" class="external-link">sample_n</a></span><span class="op">(</span><span class="fl">5</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html" class="external-link">pull</a></span><span class="op">(</span><span class="va">sentence</span><span class="op">)</span></span></code></pre></div>
<p>Inspecting each of these clusters and trying to figure out what each
cluster means and how they inter-relate would soon become intractable
for humans. Thankfully, within BERTopic there are quantitative methods
already in place to aid this procedure.</p>
<p>Instead of looking at the individual posts in each cluster, we’ll
attempt to summarise their contents with keywords and phrases. In order
to this, we’ll make a vectoriser and a ctfidf model. After creating
these models, we can use everything we’ve looked at so far to compile a
model, fit the model on our data, and finally explore our topics and
their representations.</p>
</div>
</div>
<div class="section level3">
<h3 id="adjust-the-representation">Adjust the Representation<a class="anchor" aria-label="anchor" href="#adjust-the-representation"></a>
</h3>
<div class="section level4">
<h4 id="make-the-vectoriser">Make the vectoriser<a class="anchor" aria-label="anchor" href="#make-the-vectoriser"></a>
</h4>
<p>For the vectoriser we’ll set the ngram range as c(1, 2) this means
our topics can be represented as single words or bigrams. We’ll set
stop_words to ‘english’ so that English stop words are removed and we’ll
tell our vectoriser to only consider words that have a frequency of 3 or
higher, so that rare words and chance occurrences don’t clog our
representations too much. In practice, we will want to set a higher
value for min_frequency as we’ll be working with significantly more
data.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vectoriser</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_vectoriser.html">bt_make_vectoriser</a></span><span class="op">(</span>ngram_range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span>, stop_words <span class="op">=</span> <span class="st">"english"</span>, min_frequency <span class="op">=</span> <span class="fl">3L</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="make-the-ctfidf-model">Make the ctfidf model<a class="anchor" aria-label="anchor" href="#make-the-ctfidf-model"></a>
</h4>
<p>Then we’ll create a ctfidf model which will allow us to represent
each topic according to the words that are important to that topic (have
high frequency) and distinct to that topic (have relatively low
frequency in other topics):</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ctfidf</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_make_ctfidf.html">bt_make_ctfidf</a></span><span class="op">(</span>reduce_frequent_words <span class="op">=</span> <span class="cn">TRUE</span>, bm25_weighting <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level3">
<h3 id="compile-the-model">Compile the model<a class="anchor" aria-label="anchor" href="#compile-the-model"></a>
</h3>
<p>We’ve already made our individual components, or modules, and
selected their parameters. We’ve already performed the embeddings and
dimensionality reduction, so bertopic allows us to skip these steps
easily by feeding in empty models to the <code>bt_compile_model</code>
function for embedding and reducing. We can also skip clustering, but
won’t for this task as it adds extra complexity, the clustering we
performed above with bt_do_clustering was just to explore how our
clusterer would work in practice.</p>
<p>N.B. In practice you will need to pause and explore your parameters
in more depth.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span></span>
<span><span class="va">topic_model</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_compile_model.html">bt_compile_model</a></span><span class="op">(</span></span>
<span>  embedding_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_empty_embedder.html">bt_empty_embedder</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  reduction_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_empty_reducer.html">bt_empty_reducer</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  clustering_model <span class="op">=</span> <span class="va">clusterer</span>,</span>
<span>  vectoriser_model <span class="op">=</span> <span class="va">vectoriser</span>,</span>
<span>  ctfidf_model <span class="op">=</span> <span class="va">ctfidf</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="fit-the-model">Fit the model<a class="anchor" aria-label="anchor" href="#fit-the-model"></a>
</h3>
<p>We feed in our reduced embeddings rather than the original
embeddings, this allows us to skip steps in the workflow; this can save
us a lot of time, particularly when we have many documents.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/bt_fit_model.html">bt_fit_model</a></span><span class="op">(</span><span class="va">topic_model</span>, <span class="va">data</span><span class="op">$</span><span class="va">sentence</span>, embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span><span class="op">)</span></span>
<span></span>
<span><span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span></code></pre></div>
<p><strong>NOTE:</strong> The bertopic model you are working with is a
pointer to a python object at a point in memory. This means that the
input and the output model cannot be differentiated between without
explicitly saving the model before performing this operation. We do not
need to specify an output to the bt_fit_model function as the function
changes the input model in place. Note the output of topic_model$topics_
after the bt_compile_model() function and the bt_fit_model() function,
before calling bt_fit_model(), when the model has not yet been fitted to
any data, the model’s topics attribute is NULL, after being fitted, the
topics attribute contains a list of topics related to the documents to
which the model was fitted. Similarly, if we assigned an output to
bt_fit_model, the output model would be the same as the input model:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">example_model_compiled</span> <span class="op">&lt;-</span>  <span class="fu"><a href="../reference/bt_compile_model.html">bt_compile_model</a></span><span class="op">(</span></span>
<span>  embedding_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_empty_embedder.html">bt_empty_embedder</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  reduction_model <span class="op">=</span> <span class="fu"><a href="../reference/bt_empty_reducer.html">bt_empty_reducer</a></span><span class="op">(</span><span class="op">)</span>,</span>
<span>  clustering_model <span class="op">=</span> <span class="va">clusterer</span>,</span>
<span>  vectoriser_model <span class="op">=</span> <span class="va">vectoriser</span>,</span>
<span>  ctfidf_model <span class="op">=</span> <span class="va">ctfidf</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">example_model_compiled</span><span class="op">$</span><span class="va">topics_</span></span>
<span></span>
<span><span class="va">example_model_fitted</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bt_fit_model.html">bt_fit_model</a></span><span class="op">(</span><span class="va">example_model_compiled</span>, <span class="va">data</span><span class="op">$</span><span class="va">sentence</span>, embeddings <span class="op">=</span> <span class="va">reduced_embeddings</span><span class="op">)</span></span>
<span></span>
<span><span class="va">example_model_compiled</span><span class="op">$</span><span class="va">topics_</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span>
<span><span class="va">example_model_fitted</span><span class="op">$</span><span class="va">topics_</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span></code></pre></div>
<p>We can create a look up table to join our sentences to their topic
labels and their topic descriptions and join this information with our
original dataframe:</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">topic_representations</span> <span class="op">&lt;-</span> <span class="va">topic_model</span><span class="op">$</span><span class="fu">get_topic_info</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">topic_rep_lookup</span> <span class="op">&lt;-</span> <span class="va">topic_representations</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/select.html" class="external-link">select</a></span><span class="op">(</span>topic <span class="op">=</span> <span class="va">Topic</span>, description <span class="op">=</span> <span class="va">Name</span>, topic_size <span class="op">=</span> <span class="va">Count</span><span class="op">)</span></span>
<span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate.html" class="external-link">mutate</a></span><span class="op">(</span>topic <span class="op">=</span> <span class="va">topic_model</span><span class="op">$</span><span class="va">topics_</span><span class="op">)</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/mutate-joins.html" class="external-link">left_join</a></span><span class="op">(</span><span class="va">topic_rep_lookup</span><span class="op">)</span></span>
<span></span>
<span><span class="op">(</span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">data</span> <span class="op"><a href="../reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/relocate.html" class="external-link">relocate</a></span><span class="op">(</span><span class="va">sentence</span>, <span class="va">topic</span>, <span class="va">topic_size</span>, <span class="va">description</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The df compiled is a common place for everything we have generated so
far, in practice we don’t really need the cluster column now that we
have the topic column, similarly, they provide the same information.
Topics and clusters should have largely the same labels, the only
discrepancy would be where multiple clusters/topics are the same
size.</p>
<p>Now that you have your model up and running, you could look at the <a href="manipulating-the-model.html">Manipulating the Model</a> vignette
to see how you could investigate and alter the topics that have been
identified.</p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Jack Penzer, Aoife Ryan.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
