[{"path":"/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2023 BertopicR authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"/articles/manipulating-the-model.html","id":"changing-the-model-representation","dir":"Articles","previous_headings":"","what":"Changing the Model Representation","title":"Manipulating the Model","text":"happy topics/clusters formed, methods can use improve topic representations get better understanding topic . representation methods currently available : KeyBERT keyword extraction technique uses BERT embeddings represent topics appropriate keywords phrases. MaximalMarginalRelevance concept used select relevant keywords phrases promoting diversity keywords. balances relevance topic distinctiveness previously chosen keywords phrases using trade-parameter called lambda. OpenAI allows us use available models generate topic summaries. OpenAI API key required access api models (set use Sys.setenv(“OPENAI_API_KEY” = “sk-”)). HuggingFace allows us use available models generate topic summaries. Unlike OpenAI, need API key completely free. However, models sophisticated OpenAI’s. Now trialled representation methods, can look compare default representations able get good idea topic . notice gpt-3.5 model gives coherent topic representation easy just take gospel chose topic title based . important remember, like representation methods, number input nr_repr_docs bt_representation_openai sent model large topic, documents may represent topic whole.","code":"#> UMAP(low_memory=False, min_dist=0, n_components=5, n_neighbors=10, random_state=42, verbose=True) #> Thu Oct 26 11:30:19 2023 Construct fuzzy simplicial set #> Thu Oct 26 11:30:19 2023 Finding Nearest Neighbors #> Thu Oct 26 11:30:21 2023 Finished Nearest Neighbor Search #> Thu Oct 26 11:30:23 2023 Construct embedding #> Thu Oct 26 11:30:24 2023 Finished embedding representation_keybert <- bt_representation_keybert(fitted_model = topic_model,                                                     documents = sentences,                                                     document_embeddings = embeddings,                                                     embedding_model = embedder,                                                     top_n_words = 10,                                                     nr_repr_docs = 50,                                                     nr_samples = 500,                                                     nr_candidate_words = 100)  representation_mmr <- bt_representation_mmr(fitted_model = topic_model,                                             embedding_model = embedder,                                             diversity = 0.5)  representation_openai <- bt_representation_openai(fitted_model = topic_model,                                                   documents = sentences,                                                   openai_model = \"gpt-3.5-turbo\",                                                   nr_repr_docs = 10,                                                   chat = TRUE,                                                   api_key = \"sk-\")  representation_hf <- bt_representation_hf(fitted_model = topic_model,                                           documents = sentences,                                           task = \"text2text-generation\",                                           hf_model = \"google/flan-t5-base\",                                           default_prompt = \"keywords\") topic_representations <- topic_model$get_topic_info() %>%   mutate(keybert = representation_keybert,          mmr = representation_mmr,          # openai = representation_openai,          flanT5 = representation_hf) %>%   select(-Representative_Docs)  topic_representations %>% select(-Topic, -Count, -Name) %>%   mutate(keybert = stringr::str_replace_all(keybert, \"_\",\", \"),          mmr = stringr::str_replace_all(mmr, \"_\",\", \"),          Representation = stringr::str_replace_all(Representation, \"_\",\", \")) %>%   DT::datatable(options = list(scrollX = TRUE)) #> Warning: There was 1 warning in `mutate()`. #> ℹ In argument: `Representation = stringr::str_replace_all(Representation, \"_\", #>   \", \")`. #> Caused by warning in `stri_replace_all_regex()`: #> ! argument is not an atomic vector; coercing"},{"path":"/articles/manipulating-the-model.html","id":"modifying-topics","dir":"Articles","previous_headings":"","what":"Modifying Topics","title":"Manipulating the Model","text":"Two biggest inconveniences using hdbscan clustering introduces generation large numbers clusters (topics) presence can huge numbers outliers. order topic analysis practical digestible, likely want reduce number topics , depending use case, may want reduce number outliers.","code":""},{"path":"/articles/manipulating-the-model.html","id":"merging-topics","dir":"Articles","previous_headings":"Modifying Topics","what":"Merging Topics","title":"Manipulating the Model","text":"Particularly using hdbscan can end large number topics can useful merge topics think suitably similar. can get certain idea topic descriptions already generated, can also useful look data closely merging.","code":""},{"path":"/articles/manipulating-the-model.html","id":"hierarchical-clustering","dir":"Articles","previous_headings":"Modifying Topics > Merging Topics","what":"Hierarchical Clustering","title":"Manipulating the Model","text":"Hdbscan clustering forms clusters hierarchical processes can visualise dendrogram. can useful merging topics can see clusters split become topics emerged topic modelling process. x-axis measure distance topic embeddings, clusters split higher x-value larger distance embeddings. can see particular dataset, clusters split final topics quite early hierarchy might appropriate merge topics based emerged hierarchy. hierarchical structure based topics emerge based similarity embeddings, however, can often find topics think merged based knowledge. example, despite embeddings relatively large distance , topic 2 14 appear food.","code":"hierarchical_topics <- topic_model$hierarchical_topics(sentences) topic_model$visualize_hierarchy(hierarchical_topics = hierarchical_topics)$show() #>    0%|          | 0/64 [00:00<?, ?it/s]  20%|##        | 13/64 [00:00<00:00, 127.96it/s]  48%|####8     | 31/64 [00:00<00:00, 158.37it/s]  81%|########1 | 52/64 [00:00<00:00, 178.80it/s] 100%|##########| 64/64 [00:00<00:00, 179.37it/s]"},{"path":"/articles/manipulating-the-model.html","id":"looking-at-topic-contents","dir":"Articles","previous_headings":"Modifying Topics > Merging Topics","what":"Looking at Topic Contents","title":"Manipulating the Model","text":"larger topics might need use sophisticated language analysis tools, since topics relatively small, can just examine exemplars. pretty happy two topics merged larger “food” topic, use bt_merge_topics function: maintaining dataframe along tracking step ’ve completed, good now update dataframe new topics.","code":"topic_representations %>%   filter(Topic %in% c(2,14)) #>   Topic Count                     Name #> 1     2    17 2_smell_cloth_clean_dirt #> 2    14    11    14_big_torn_stop_port #>                                                        Representation #> 1 smell, cloth, clean, dirt, times, tender, neat, burned, bring, dust #> 2       big, torn, stop, port, brought, blow, sharp, lost, cut, clear #>                                                      keybert #> 1 burned_sharp_dirt_cloth_dust_floor_tender_water_makes_deep #> 2    cut_water_sharp_clear_small_hard_blow_torn_brought_lost #>                                                          mmr #> 1 dirt_cloth_dust_clean_tender_burned_neat_bring_times_smell #> 2       sharp_cut_port_torn_clear_blow_lost_stop_brought_big #>                             flanT5 #> 1 bringing a smoky smell to a room #> 2       a splinter cut to the port data %>%   filter(topic %in% c(2,14)) %>%   select(sentence, topic) #> # A tibble: 28 × 2 #>    sentence                                           topic #>    <chr>                                              <int> #>  1 wipe the grease off his dirty face.                    2 #>  2 the ship was torn apart on the sharp reef.            14 #>  3 the navy attacked the big task force.                 14 #>  4 the fin was sharp and cut the clear water.            14 #>  5 bail the boat to stop it from sinking.                14 #>  6 a cramp is no small danger on a swim.                 14 #>  7 mud was spattered on the front of his white shirt.     2 #>  8 the pirates seized the crew of the lost ship.         14 #>  9 a rag will soak up spilled water.                      2 #> 10 they felt gay when the ship arrived in port.          14 #> # ℹ 18 more rows bt_merge_topics(fitted_model = topic_model,                 documents = sentences,                 topics_to_merge = list(2, 14)) #>  #> Topics merged & input model updated accordingly data <- data %>%   mutate(merged_topics = topic_model$topics_)"},{"path":"/articles/manipulating-the-model.html","id":"reducing-outliers","dir":"Articles","previous_headings":"Modifying Topics","what":"Reducing Outliers","title":"Manipulating the Model","text":"One feature hdbscan outlier category, can quite large. Sometimes might want redistribute outlier documents fall within one existing topics. number methods achieve good practice look different parameters different methods reducing outliers can quite difficult redistribute outlier documents maintaining clarity within topics. end, consider project goal implementing methods, important concise coherent topics force /documents topics, balance two? methods currently available us : Tokenset Similarity: Divides documents tokensets calculates c-TF-IDF cosine similarity tokenset topic. summation cosine similarity score topic across outlier document gives similar topic outlier document. Embeddings: Measures cosine similarity embeddings outlier document topic. passed empty embedding model bt_compile_model (), must specify embedding model used function. c-TF-IDF: Calculates c-TF-IDF cosine similarity outlier document topic redistributes outliers based topic highest similarity. can play outlier strategies , unlike merge topics fit model, bt_outlier_* functions update model, output df document, current topic classification potential new topics. must update model using bt_update_topics actually change topics within model. useful now look method redistributed outlier topics. graph shows outliers redistributed topics topic 12. can see strategy redistribute topics way, embedding strategy example, found 6 outlier documents best represented topic 1, strategy found outlier documents best represented topic 1. embedding method also redistributed outlier documents, c-TF-IDF tokenset similarity methods left certain documents outliers. playing around threshold parameter, find good fit data chosen strategy, important.  take look documents redistributed topic redistributed deciding best strategy data. Unfortunately, can quite laborious large amounts data many topics. settled new list topics happy , can update dataframe keeping. example, looking data decided Tokenset Similarity method appropriate: can update model new topics, first consider future use model, intention use model fit new data, better fit based original, selective topic classification less selective classification outlier reduction resulted ? like deeper look else can using bertopic, refer BertopicR function documentation BERTopic python library, https://maartengr.github.io/BERTopic/index.html.","code":"outliers_ts_sim <- bt_outliers_tokenset_similarity(fitted_model = topic_model,                                                    documents = sentences,                                                    topics = topic_model$topics_,                                                    threshold = 0.1)  outliers_embed <- bt_outliers_embeddings(fitted_model = topic_model,                                          documents = sentences,                                          topics = topic_model$topics_,                                          embeddings = reduced_embeddings,                                          embedding_model = embedder,                                          threshold = 0.1)  outliers_ctfidf <- bt_outliers_ctfidf(fitted_model = topic_model,                                       documents = sentences,                                       topics = topic_model$topics_,                                       threshold = 0.1) data %>%   mutate(outliers_ts_sim = outliers_ts_sim$new_topics,          outliers_embed = outliers_embed$new_topics,          outliers_ctfidf = outliers_ctfidf$new_topics) %>%   filter(merged_topics == -1,          outliers_ctfidf < 12,          outliers_embed < 12,          outliers_ts_sim < 12) %>%   select(outliers_ts_sim, outliers_embed, outliers_ctfidf) %>%   pivot_longer(everything(), names_to = \"outlier_distribution_strategy\", values_to = \"topic\") %>%   ggplot(aes(x = as.factor(topic), fill = outlier_distribution_strategy)) +   geom_bar(position = position_dodge2(preserve = \"single\")) +   theme_minimal() +   labs(x = \"Numbers\",        y = \"Count\",        title = \"Number of outliers in each topic after redistribution\",        fill = \"Outlier redistribution strategy\") +   scale_fill_discrete(labels = c(outliers_ctfidf = \"c-TF-IDF\",                                outliers_embed = \"Embeddings\",                                outliers_ts_sim = \"Tokenset similarity\")) data <- data %>%   mutate(new_topics = outliers_ts_sim$new_topics)  data %>%   filter(merged_topics == -1) %>%   select(merged_topics, new_topics) #> # A tibble: 189 × 2 #>    merged_topics new_topics #>            <int>      <dbl> #>  1            -1         -1 #>  2            -1         46 #>  3            -1         -1 #>  4            -1         59 #>  5            -1         -1 #>  6            -1         41 #>  7            -1         -1 #>  8            -1         34 #>  9            -1         56 #> 10            -1         -1 #> # ℹ 179 more rows bt_update_topics(fitted_model = topic_model,                  documents = sentences,                  new_topics = outliers_ts_sim$new_topics) #>  #> Input model updated"},{"path":"/articles/modular_approach.html","id":"modules","dir":"Articles","previous_headings":"","what":"Modules","title":"Interacting with Individual Modules","text":"Bertopic Python Library Maarten Grootendorst 6 sub-modules: Embeddings - transforming documents numerical representation Dimensionality Reduction - reducing number features embeddings output Clustering - finding groups similar documents represent topics Vectorisers - find n-grams describe topic c-TF-IDF - create topic-level (rather document) bag words matrices representing topics Fine-tuning topic representations - tools topic representations (includes generative AI/LLMs), can read Manipulating Model vignette.  vignette show use {BertopicR} tune module creating topic models.","code":""},{"path":"/articles/modular_approach.html","id":"data","dir":"Articles","previous_headings":"Modules","what":"Data","title":"Interacting with Individual Modules","text":"demonstrative purposes, ’ll use {stringr}‘s ’sentences’ data set, comes fairly clean. help cleaning text data visit ParseR/LimpiaR documentation. Let’s take look first five posts brevity. ’ll turn sentences data frame:","code":"sentences <- stringr::sentences sentences[1:5] #> [1] \"The birch canoe slid on the smooth planks.\"  #> [2] \"Glue the sheet to the dark blue background.\" #> [3] \"It's easy to tell the depth of a well.\"      #> [4] \"These days a chicken leg is a rare dish.\"    #> [5] \"Rice is often served in round bowls.\" df <- dplyr::tibble(sentences = tolower(sentences))"},{"path":"/articles/modular_approach.html","id":"embeddings","dir":"Articles","previous_headings":"Modules","what":"Embeddings","title":"Interacting with Individual Modules","text":"order work efficiently text data, need turn words numbers. current state---art approach turning text numbers, contextualised word embeddings. ’ll use MpNet model, ‘-mpnet-base-v2’ take sentences turn numbers (embeddings). allow us find similarities differences sentences, using standard mathematical techniques (don’t worry isn’t making sense right now, often best way learn ). BertopicR usually either want making modules, , compiling fitting models. make components, actions data components. compile components models, fit models data.","code":""},{"path":"/articles/modular_approach.html","id":"make-the-embedder","dir":"Articles","previous_headings":"Modules > Embeddings","what":"Make the embedder","title":"Interacting with Individual Modules","text":"First ’ll make embedder (embedding_model) using bt_make_embedder_st function. ’ll embed sentences using embedder. TIP: ’s good idea save embeddings, working many documents process time consuming.","code":"embedder <- BertopicR::bt_make_embedder_st(   model = \"all-mpnet-base-v2\"   )"},{"path":"/articles/modular_approach.html","id":"do-the-embedding","dir":"Articles","previous_headings":"Modules > Embeddings","what":"Do the embedding","title":"Interacting with Individual Modules","text":"row embeddings output represents one original sentences, column represents different embedding dimension; 768 dimensions outputted ‘-mpnet-base-v2’ mode take peek first 10 columns (dimensions), first row embeddings see 10 floating point numbers.","code":"embeddings <- bt_do_embedding(   embedder,   df$sentences,   accelerator = NULL   ) #>  #> Embedding proccess finished #> all-mpnet-base-v2 added to embeddings attributes  embeddings[1, 1:10] #>  [1]  0.0031732682 -0.0454455093 -0.0003524341  0.0046989778  0.0179691389 #>  [6] -0.0300240405 -0.0278542358 -0.0199615862 -0.0032402310  0.0257827044"},{"path":"/articles/modular_approach.html","id":"reducing-dimensions","dir":"Articles","previous_headings":"Modules","what":"Reducing Dimensions","title":"Interacting with Individual Modules","text":"next step pipeline reduce dimensions embeddings, two reasons: allow clustering algorithm run smoothly visualise clusters plane (eventually reduce 2 dimensions) machine learning generally, dimensionality reduction often important step avoid overfitting curse dimensionality. example use UMAP algorithm (however dimensionality reduction using PCA truncatedSVD also currently available) information UMAP algorithm - uniform manifold approximation projection dimension reduction - catchy, see UMAP Docs. TIP: Like embeddings, ’s good idea save reduced embeddings, reducing dimensions can costly process.","code":""},{"path":"/articles/modular_approach.html","id":"make-the-reducer","dir":"Articles","previous_headings":"Modules > Reducing Dimensions","what":"Make the reducer","title":"Interacting with Individual Modules","text":"’ll use low-ish value n_neighbours (small dataset) output 5 dimensions (n_components = 5L). ’ll set min_distance 0, dimensionality reduction model can place similar documents close together. ’ll set metric “Euclidean” see Embedding non-Euclidean Spaces alternatives.","code":"reducer <- bt_make_reducer_umap(   n_neighbours = 10L,   n_components = 5L,   min_dist = 0L,   metric = \"euclidean\"   )"},{"path":"/articles/modular_approach.html","id":"do-the-reducing","dir":"Articles","previous_headings":"Modules > Reducing Dimensions","what":"Do the reducing","title":"Interacting with Individual Modules","text":"’ll take peek two rows now represent reduced dimension embeddings document. Notice numbers floating points, also bounded -1 1. next step cluster data. first pass, bertopic considers discovered cluster topic. Choice clustering model selected parameters therefore important. ’ll use hdbscan cluster, ’s bertopic initially built .","code":"reduced_embeddings <- bt_do_reducing(   reducer, embeddings = embeddings ) #> UMAP(low_memory=False, min_dist=0, n_components=5, n_neighbors=10, random_state=42, verbose=True) #> Thu Oct 26 11:32:09 2023 Construct fuzzy simplicial set #> Thu Oct 26 11:32:09 2023 Finding Nearest Neighbors #> Thu Oct 26 11:32:11 2023 Finished Nearest Neighbor Search #> Thu Oct 26 11:32:13 2023 Construct embedding #> Thu Oct 26 11:32:14 2023 Finished embedding  reduced_embeddings[1:2, ] #>          [,1]     [,2]     [,3]     [,4]      [,5] #> [1,] 5.860952 3.167575 8.717437 6.168198 -2.154495 #> [2,] 4.798744 4.661528 6.752989 6.723210 -2.855127"},{"path":"/articles/modular_approach.html","id":"clustering","dir":"Articles","previous_headings":"Modules","what":"Clustering","title":"Interacting with Individual Modules","text":"lot learn comes clustering, selecting correct parameters notoriously difficult - especially clustering without pre-assigned labels, clustering tends . run ’ll use hdbscan clustering algorithm, don’t know many clusters look advance (using kMeans clustering example).hdbscan documentation ’s important know get level updating topic representations, bertopic pipeline 1 topic = 1 cluster. ’s therefore crucial gather information can data inform clustering process.","code":""},{"path":"/articles/modular_approach.html","id":"make-the-clusterer","dir":"Articles","previous_headings":"Modules > Clustering","what":"Make the clusterer","title":"Interacting with Individual Modules","text":"’ll stick Euclidean distance metric, ’ll reduce min_cluster_size 10, giving us theoretical maximum number clusters : length(sentences) / 10 min_samples equal 5. relationship min_cluster_size min_samples important, default min_samples = min_cluster_size specified, likely adverse effects clustering outputs dealing larger datasets (’ll likely want raise min_cluster_size parameter significantly). hand, hdbscan documentation claims min_samples, parameter inherited dbscan, importance hdbscan algorithm - though also say remains algorithm’s biggest weakness. ’ll also set cluster_selection_method = “leaf”, means ’ll tend find many small clusters, rather large clusters. another parameter fraught danger, get right first time unlikely, likely require trial error, least beginning.","code":"clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 5L,                           metric = \"euclidean\",                           cluster_selection_method = \"leaf\",                           min_samples = 3L)"},{"path":"/articles/modular_approach.html","id":"do-the-clustering","dir":"Articles","previous_headings":"Modules > Clustering","what":"Do the clustering","title":"Interacting with Individual Modules","text":"clusters now list cluster labels, 720 labels total - one sentence {Stringr}’s sentences data set. cluster labels output integers, ’s important assume work like regular integers . ’s necessarily case cluster 1 closer cluster 4 cluster 15, ordering labels can effectively considered random. likely labels training/test/validation data set, rely inspecting clusters, remembering bertopic 1 cluster = 1 topic. check whether clusters make sense, draw upon data analysis & visualisation tool kit, inspect cluster every . soon become intractable. Instead, ’ll take quick look distribution ’ll use bt_compile_model() bt_fit_model() functions get topic models .","code":"clusters <- bt_do_clustering(clustering_model = clusterer, embeddings = reduced_embeddings)"},{"path":"/articles/modular_approach.html","id":"create-a-data-frame","dir":"Articles","previous_headings":"Modules > Clustering","what":"Create a data frame","title":"Interacting with Individual Modules","text":"first, ’re beginning acquire bunch objects may become hard maintain. can store data frame: want save data frame, ’ll need save .Rdata/.rds object, .csv .xlsx contains list columns.","code":"data <- dplyr::tibble(sentence = tolower(sentences)) %>%   mutate(embeddings = list(embeddings),          reduced_embeddings = list(reduced_embeddings),          cluster = as.integer(clusters))"},{"path":"/articles/modular_approach.html","id":"count-the-clusters","dir":"Articles","previous_headings":"Modules > Clustering","what":"Count the clusters","title":"Interacting with Individual Modules","text":"can see distribution via histogram:  exception outlier group, clusters labelled order size: 213/720 (29.6%) data points labelled noise (cluster == -1), upon first inspection appear quite eclectic. clusters look? clusters look , difficult become figure ’s happening… Inspecting clusters trying figure cluster means inter-relate soon become intractable humans. Thankfully, within BERTopic quantitative methods already place aid procedure. Instead looking individual posts cluster, ’ll attempt summarise contents keywords phrases. order , ’ll make vectoriser ctfidf model. creating models, can use everything ’ve looked far compile model, fit model data, finally explore topics representations.","code":"library(ggplot2)  data %>%   filter(cluster != -1) %>%   count(cluster, sort = TRUE) %>%   ggplot(aes(x= n)) +   geom_histogram(fill = \"midnightblue\", bins = 20) +   theme_minimal() +   xlab(\"Cluster Size\") +   ylab(\"Number of Clusters\") data %>%   count(cluster, sort = TRUE) #> # A tibble: 62 × 2 #>    cluster     n #>      <int> <int> #>  1      -1   221 #>  2       0    22 #>  3       1    16 #>  4       2    15 #>  5       3    13 #>  6       4    13 #>  7       5    13 #>  8       6    13 #>  9       7    12 #> 10       8    12 #> # ℹ 52 more rows data %>%   filter(cluster == -1) %>%   slice(30:40) %>%   pull(sentence) #>  [1] \"the heart beat strongly and with firm strokes.\" #>  [2] \"cut the pie into large parts.\"                  #>  [3] \"he lay prone and hardly moved a limb.\"          #>  [4] \"the fin was sharp and cut the clear water.\"     #>  [5] \"the play seems dull and quite stupid.\"          #>  [6] \"open the crate but don't break the glass.\"      #>  [7] \"thieves who rob friends deserve jail.\"          #>  [8] \"the ripe taste of cheese improves with age.\"    #>  [9] \"act on these orders with great speed.\"          #> [10] \"split the log with a quick, sharp blow.\"        #> [11] \"weave the carpet on the right hand side.\" data %>%   filter(cluster == 0) %>%   sample_n(5) %>%   pull(sentence) #> [1] \"the child almost hurt the small dog.\"               #> [2] \"the hog crawled under the high fence.\"              #> [3] \"dots of light betrayed the black cat.\"              #> [4] \"see the cat glaring at the scared mouse.\"           #> [5] \"the pup jerked the leash as he saw a feline shape.\" data %>%   filter(cluster == 1) %>%   sample_n(5) %>%   pull(sentence) #> [1] \"eight miles of woodland burned to waste.\"     #> [2] \"the man went to the woods to gather sticks.\"  #> [3] \"the red paper brightened the dim stage.\"      #> [4] \"they took the axe and the saw to the forest.\" #> [5] \"the heap of fallen leaves was set on fire.\" data %>%   filter(cluster == 2) %>%   sample_n(5) %>%   pull(sentence) #> [1] \"no doubt about the way the wind blows.\"     #> [2] \"those thistles bend in a high wind.\"        #> [3] \"the tree top waved in a graceful way.\"      #> [4] \"the leaf drifts along with a slow spin.\"    #> [5] \"a vent near the edge brought in fresh air.\""},{"path":[]},{"path":"/articles/modular_approach.html","id":"make-the-vectoriser","dir":"Articles","previous_headings":"Modules > Adjust the Representation","what":"Make the vectoriser","title":"Interacting with Individual Modules","text":"vectoriser ’ll set ngram range c(1, 2) means topics can represented single words bigrams. ’ll set stop_words ‘english’ English stop words removed ’ll tell vectoriser consider words frequency 3 higher, rare words chance occurrences don’t clog representations much. practice, want set higher value min_frequency ’ll working significantly data.","code":"vectoriser <- bt_make_vectoriser(ngram_range = c(1, 2), stop_words = \"english\", min_frequency = 3L)"},{"path":"/articles/modular_approach.html","id":"make-the-ctfidf-model","dir":"Articles","previous_headings":"Modules > Adjust the Representation","what":"Make the ctfidf model","title":"Interacting with Individual Modules","text":"’ll create ctfidf model allow us represent topic according words important topic (high frequency) distinct topic (relatively low frequency topics):","code":"ctfidf <- bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)"},{"path":"/articles/modular_approach.html","id":"compile-the-model","dir":"Articles","previous_headings":"Modules","what":"Compile the model","title":"Interacting with Individual Modules","text":"’ve already made individual components, modules, selected parameters. ’ve already performed embeddings dimensionality reduction, bertopic allows us skip steps easily feeding empty models bt_compile_model function embedding reducing. can also skip clustering, won’t task adds extra complexity, clustering performed bt_do_clustering just explore clusterer work practice. N.B. practice need pause explore parameters depth.","code":"topic_model <- bt_compile_model(   embedding_model = bt_empty_embedder(),   reduction_model = bt_empty_reducer(),   clustering_model = clusterer,   vectoriser_model = vectoriser,   ctfidf_model = ctfidf ) #>  #> Model built  topic_model$topics_ #> NULL"},{"path":"/articles/modular_approach.html","id":"fit-the-model","dir":"Articles","previous_headings":"Modules","what":"Fit the model","title":"Interacting with Individual Modules","text":"feed reduced embeddings rather original embeddings, allows us skip steps workflow; can save us lot time, particularly many documents. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. need specify output bt_fit_model function function changes input model place. Note output topic_model$topics_ bt_compile_model() function bt_fit_model() function, calling bt_fit_model(), model yet fitted data, model’s topics attribute NULL, fitted, topics attribute contains list topics related documents model fitted. Similarly, assigned output bt_fit_model, output model input model: can create look table join sentences topic labels topic descriptions join information original dataframe: df compiled common place everything generated far, practice don’t really need cluster column now topic column, similarly, provide information. Topics clusters largely labels, discrepancy multiple clusters/topics size. Now model running, look Manipulating Model vignette see investigate alter topics identified.","code":"bt_fit_model(topic_model, data$sentence, embeddings = reduced_embeddings) #>  #> The input model has bee fitted to the data and updated accordingly  topic_model$topics_[1:10] #>  [1] -1 -1  3 28 28 21 -1 -1 -1 -1 example_model_compiled <-  bt_compile_model(   embedding_model = bt_empty_embedder(),   reduction_model = bt_empty_reducer(),   clustering_model = clusterer,   vectoriser_model = vectoriser,   ctfidf_model = ctfidf ) #>  #> Model built  example_model_compiled$topics_ #> NULL  example_model_fitted <- bt_fit_model(example_model_compiled, data$sentence, embeddings = reduced_embeddings) #>  #> The input model has bee fitted to the data and updated accordingly  example_model_compiled$topics_[1:10] #>  [1] -1 -1  3 28 28 21 -1 -1 -1 -1 example_model_fitted$topics_[1:10] #> NULL topic_representations <- topic_model$get_topic_info() topic_rep_lookup <- topic_representations %>%   select(topic = Topic, description = Name, topic_size = Count)  data <- data %>%   mutate(topic = topic_model$topics_) %>%   left_join(topic_rep_lookup) #> Joining with `by = join_by(topic)`  (data <- data %>%   relocate(sentence, topic, topic_size, description) ) #> # A tibble: 720 × 7 #>    sentence   topic topic_size description embeddings reduced_embeddings cluster #>    <chr>      <dbl>      <dbl> <chr>       <list>     <list>               <int> #>  1 the birch…    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #>  2 glue the …    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #>  3 it's easy…     3         13 3_ice_stea… <dbl[…]>   <dbl [720 × 5]>          5 #>  4 these day…    28          7 28_rare_se… <dbl[…]>   <dbl [720 × 5]>         28 #>  5 rice is o…    28          7 28_rare_se… <dbl[…]>   <dbl [720 × 5]>         28 #>  6 the juice…    21          9 21_tree_se… <dbl[…]>   <dbl [720 × 5]>         19 #>  7 the box w…    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #>  8 the hogs …    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #>  9 four hour…    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #> 10 a large s…    -1        221 -1_large_r… <dbl[…]>   <dbl [720 × 5]>         -1 #> # ℹ 710 more rows"},{"path":"/articles/quick_start.html","id":"preparing-the-data","dir":"Articles","previous_headings":"","what":"Preparing the Data","title":"Topic Modelling without Optimisation","text":"First load data like fit model.","code":"library(BertopicR) library(dplyr) library(stringr) library(tidyr) sentences <- stringr::sentences"},{"path":"/articles/quick_start.html","id":"compiling-the-model","dir":"Articles","previous_headings":"","what":"Compiling the Model","title":"Topic Modelling without Optimisation","text":"read Modular Approach vignette, seen specified individual component topic model (embedding_model, ctfidf_model etc.) fed bt_compile_model. wish, can use entirely default parameters (combination default parameters specified components) function.","code":"model <- bt_compile_model() #>  #> No embedding model provided, defaulting to 'all-miniLM-L6-v2' model as embedder. #>  #> No reduction_model provided, using default 'bt_reducer_umap' parameters. #>  #> No clustering model provided, using hdbscan with default parameters. #>  #> No vectorising model provided, creating model with default parameters #>  #> No ctfidf model provided, creating model with default parameters #>  #> Model built"},{"path":"/articles/quick_start.html","id":"fitting-the-model","dir":"Articles","previous_headings":"","what":"Fitting the Model","title":"Topic Modelling without Optimisation","text":"Now created model uses default parameters, can simply use bt_fit_model function fit model sentences data. important note created document embeddings reduced embeddings, done internally can quite time consuming process choose run topic modelling process multiple times. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. need specify output bt_fit_model function function changes input model place. See Note Fit Model section Interacting Individual Modules vignette detail. ’s , topic model running! decided wanted adjust factors, like minimum size topic, number topics want, refer Interacting Individual Modules vignette. can also refer Manipulating Model vignette see can interpret topics reduce number outliers identified (using hdbscan (default) clustering).","code":"bt_fit_model(model, sentences) #> UMAP(low_memory=False, min_dist=0.0, n_components=5, random_state=42, verbose=True) #> Thu Oct 26 11:32:41 2023 Construct fuzzy simplicial set #> Thu Oct 26 11:32:42 2023 Finding Nearest Neighbors #> Thu Oct 26 11:32:44 2023 Finished Nearest Neighbor Search #> Thu Oct 26 11:32:46 2023 Construct embedding #> Thu Oct 26 11:32:48 2023 Finished embedding #>  #> The input model has bee fitted to the data and updated accordingly  model$get_topic_info() %>% select(-Representative_Docs, - Representation) #>    Topic Count                             Name #> 1     -1   285          -1_door_hung_floor_tall #> 2      0    70            0_sun_wind_ship_cloud #> 3      1    66            1_tea_ripe_taste_corn #> 4      2    58              2_ink_wood_box_pack #> 5      3    49         3_store_bank_sold_costly #> 6      4    37          4_men_words_fun_stories #> 7      5    30           5_smell_heat_soap_dust #> 8      6    29         6_book_wrote_seven_write #> 9      7    21           7_fell_crash_storm_log #> 10     8    20             8_cat_dog_mouse_cats #> 11     9    16         9_mark_play_score_player #> 12    10    15         10_march_end_strong_just #> 13    11    13 11_talked_music_answer_size mute #> 14    12    11   12_school_child_children_young"},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Jack Penzer. Maintainer. Aoife Ryan. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Ryan (2023). BertopicR: Wraps bertopic reticulate. https://jpcompartir.github.io/BertopicR/, https://aoiferyan-sc.github.io/BertopicR/.","code":"@Manual{,   title = {BertopicR: Wraps bertopic through reticulate},   author = {Aoife Ryan},   year = {2023},   note = {https://jpcompartir.github.io/BertopicR/, https://aoiferyan-sc.github.io/BertopicR/}, }"},{"path":"/index.html","id":"bertopicr","dir":"","previous_headings":"","what":"Wraps bertopic through reticulate","title":"Wraps bertopic through reticulate","text":"goal BertopicR allow R users access bertopic’s topic modelling suite R via Reticulate. Bertopic written developed Maarten Grootendorst (contributors!). package aim implement every feature bertopic, designed specific end users mind may experienced programmers developers. may submit issues feature requests; however, may faster go direct original, Python library excellent documentation. [BERTopic documentation] tried stay true Python library, whilst developing package ‘R feel’ .e. uses functions OOP. places changed names arguments, example Python library BERTopic() takes hdbscan_model = x, opted clustering_model = x. reason hdbscan_model = artifact early days bertopic author wants ensure code backwards compatible, package developed now ’s likely author opt clustering_model =. changes aware . package currently installs exact version bertopic - 0.15.0, features introduced version take time , may never, reach package.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Wraps bertopic through reticulate","text":"installing bertopic make sure miniconda installed, don’t: reticulate miniconda installed, can install development version BertopicR GitHub : receive message: “bertopic installed packages current environment…” run:","code":"library(reticulate) #install.packages(\"reticulate\") if you don't have this already or aren't sure how to install.  reticulate::install_miniconda() # install.packages(\"devtools\") devtools::install_github(\"jpcompartir/BertopicR\")  library(BertopicR)  #Check your environment has been loaded correctly and bertopic has been installed: BertopicR::check_python_dependencies() BertopicR::install_python_dependencies()"},{"path":"/index.html","id":"quickstart","dir":"","previous_headings":"","what":"Quickstart","title":"Wraps bertopic through reticulate","text":"BertopicR ships dataset unstructured text data bert_example_data","code":"data <- BertopicR::bert_example_data  embedder <- bt_make_embedder(\"all-minilm-l6-v2\") embeddings <- bt_do_embedding(embedder, documents = data$message,  batch_size = 16L) #>  #> Embedding proccess finished #> all-minilm-l6-v2 added to embeddings attributes   reducer <- bt_make_reducer_umap(n_neighbours = 10L, n_components = 10L, metric = \"cosine\") clusterer <- bt_make_clusterer_hdbscan(min_cluster_size = 20L, metric = \"euclidean\", cluster_selection_method = \"eom\", min_samples = 10L)  topic_model <- bt_compile_model(embedding_model = embedder,                                 reduction_model = reducer,                                 clustering_model = clusterer) #>  #> No vectorising model provided, creating model with default parameters #>  #> No ctfidf model provided, creating model with default parameters #>  #> Model built  #Fit the model fitted_model <- bt_fit_model(model = topic_model,                               documents = data$message,                               embeddings = embeddings) #> UMAP(angular_rp_forest=True, low_memory=False, metric='cosine', min_dist=0.0, n_components=10, n_neighbors=10, random_state=42, verbose=True) #> Fri Sep 15 12:58:15 2023 Construct fuzzy simplicial set #> Fri Sep 15 12:58:27 2023 Finding Nearest Neighbors #> Fri Sep 15 12:58:29 2023 Finished Nearest Neighbor Search #> Fri Sep 15 12:58:31 2023 Construct embedding #> Fri Sep 15 12:58:37 2023 Finished embedding  fitted_model$get_topic_info() %>%   dplyr::tibble() #> # A tibble: 36 × 5 #>    Topic Count Name                           Representation Representative_Docs #>    <dbl> <dbl> <chr>                          <list>         <list>              #>  1    -1  1750 -1_hispanicheritagemonth_mont… <chr [10]>     <chr [3]>           #>  2     0   278 0_beto_trump_vote_fake         <chr [10]>     <chr [3]>           #>  3     1   238 1_night_heritage night_herita… <chr [10]>     <chr [3]>           #>  4     2   168 2_dance_salsa_music_tito       <chr [10]>     <chr [3]>           #>  5     3   150 3_heritage month_month_octobe… <chr [10]>     <chr [3]>           #>  6     4   126 4_students_school_grade_proje… <chr [10]>     <chr [3]>           #>  7     5   109 5_latinx_latina_uber_latinas   <chr [10]>     <chr [3]>           #>  8     6    94 6_mdcps_mdcpscentral_celebrat… <chr [10]>     <chr [3]>           #>  9     7    90 7_good thread_thread_yup_yes   <chr [10]>     <chr [3]>           #> 10     8    90 8_hispanicheritagemonth hispa… <chr [10]>     <chr [3]>           #> # ℹ 26 more rows"},{"path":"/index.html","id":"error-messages-and-causes","dir":"","previous_headings":"","what":"Error Messages and causes","title":"Wraps bertopic through reticulate","text":"tried provide informative error messages sometimes may faced error messages arisen parent python function used. get error begins “Error py_call_impl(callable, call_args$unnamed, call_args$named) :”, can use reticulate::py_last_error() trace origin error message. Note one common cause error messages working python functions R arises user fails specify integer numbers integer type. R, integers type “numeric” default, python functions typically require explicitly specified integer type. formal function arguments, conversion dealt within function, however aware specifying extra arguments function. R, achieved placing L number using .integer() function:","code":"# numbers default to \"numeric\" class(4) #> [1] \"numeric\"  # we can force them to be \"integer\" class(4L) #> [1] \"integer\" class(as.integer(4)) #> [1] \"integer\""},{"path":"/reference/bert_example_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Example bert data — bert_example_data","title":"Example bert data — bert_example_data","text":"Example text data frame old social media API","code":""},{"path":"/reference/bert_example_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Example bert data — bert_example_data","text":"","code":"bert_example_data"},{"path":"/reference/bert_example_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Example bert data — bert_example_data","text":"data frame 4,035 rows 1 column message Text variable, common variable SegmentR functions","code":""},{"path":"/reference/bertopic_detach.html","id":null,"dir":"Reference","previous_headings":"","what":"Detach bertopic from the python session — bertopic_detach","title":"Detach bertopic from the python session — bertopic_detach","text":"Call finished topic modelling process. Although, safer may simply save work restart R session, Python session still running (far know, way safely close)","code":""},{"path":"/reference/bertopic_detach.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detach bertopic from the python session — bertopic_detach","text":"","code":"bertopic_detach()"},{"path":"/reference/bertopic_detach.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detach bertopic from the python session — bertopic_detach","text":"Nothing","code":""},{"path":"/reference/bt_compile_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Build a BERTopic model — bt_compile_model","title":"Build a BERTopic model — bt_compile_model","text":"Keep *_model = NULL proceed model made default parameters (see individual make_* function parameters). However, advisable accept default parameters model; tune model according dataset business question answering.","code":""},{"path":"/reference/bt_compile_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build a BERTopic model — bt_compile_model","text":"","code":"bt_compile_model(   ...,   embedding_model = NULL,   reduction_model = NULL,   clustering_model = NULL,   vectoriser_model = NULL,   ctfidf_model = NULL )"},{"path":"/reference/bt_compile_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build a BERTopic model — bt_compile_model","text":"... Additional arguments sent bertopic.BERTopic() embedding_model Model creating embeddings (Python object) reduction_model Model reducing embeddings' dimensions (Python object) clustering_model Model clustering (Python object) vectoriser_model Model vectorising input topic representations (Python object) ctfidf_model Model performing class-based tf-idf (ctf-idf) (Python object)","code":""},{"path":"/reference/bt_compile_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build a BERTopic model — bt_compile_model","text":"BERTopic model","code":""},{"path":"/reference/bt_compile_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build a BERTopic model — bt_compile_model","text":"","code":"if (FALSE) { # model using all default parameters model <- bt_compile_model()  # model with modular components already generated # define embedding and reduction modules and pass to bt_compile_model embedder <- bt_make_embedder_st(\"all-mpnet-base-v2\")  reducer <- bt_make_reducer_umap(n_components = 10L, n_neighbours = 20L) model <- bt_compile_model(embedding_model = embedder, reduction_model = reducer)  # Perform document embedding and reduction external to bertopic model and pass empty models to bt_compile_model embedder <- bt_make_embedder_st(\"all-mpnet-base-v2\") # embedder embeddings <- bt_do_embedding(embedder, docs, accelerator = NULL) # embeddings reducer <- bt_make_reducer_umap(n_components = 10L, n_neighbours = 20L) # reducer reduced_embeddings <- bt_do_reducing(reducer, embeddings) # reduced embeddings  # skip embedding and reduction step by passing empty models model <- bt_compile_model(embedding_model = bt_empty_embedder(), reduction_model = bt_empty_reducer())   }"},{"path":"/reference/bt_do_clustering.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster your data — bt_do_clustering","title":"Cluster your data — bt_do_clustering","text":"Cluster data","code":""},{"path":"/reference/bt_do_clustering.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster your data — bt_do_clustering","text":"","code":"bt_do_clustering(clustering_model, embeddings)"},{"path":"/reference/bt_do_clustering.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster your data — bt_do_clustering","text":"clustering_model Python object, output bt_make_clusterer* embeddings Embeddings, output bt_do_embedding bt_do_reducing","code":""},{"path":"/reference/bt_do_clustering.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cluster your data — bt_do_clustering","text":"Cluster labels document","code":""},{"path":"/reference/bt_do_clustering.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cluster your data — bt_do_clustering","text":"","code":"# create clustering model clusterer <- bt_make_clusterer_kmeans(n_clusters = 2)  # mock embeddings embeddings_test <-  matrix(runif(50), nrow = 25, ncol = 2)  # create clusters clusters <- bt_do_clustering(clusterer, embeddings_test)"},{"path":"/reference/bt_do_embedding.html","id":null,"dir":"Reference","previous_headings":"","what":"Embed your documents — bt_do_embedding","title":"Embed your documents — bt_do_embedding","text":"Takes document, list documents, returns numerical embedding can used features machine learning model semantic similarity search. pre-computed embeddings can skip step. bt_embed function designed used one step topic modelling pipeline.","code":""},{"path":"/reference/bt_do_embedding.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embed your documents — bt_do_embedding","text":"","code":"bt_do_embedding(   embedder,   documents,   ...,   accelerator = NULL,   progress_bar = TRUE )"},{"path":"/reference/bt_do_embedding.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embed your documents — bt_do_embedding","text":"embedder embedding model (output bt_make_embedder) documents character vector documents embedded, e.g. text variable ... Optional additional parameters passed SentenceTransformer's encode function, e.g. batch_size accelerator string containing name hardware accelerator, e.g. \"mps\", \"cuda\". currently applied embedder sentence transformer flair library. NULL accelerator used sentence transformer flair embeddings. GPU usage spacy embeddings specified embedder creation (bt_make_embedder_spacy) progress_bar logical value indicating whether progress bar shown console. used using embedder sentence-transformer package","code":""},{"path":"/reference/bt_do_embedding.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Embed your documents — bt_do_embedding","text":"array floating point numbers","code":""},{"path":"/reference/bt_do_embedding.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Embed your documents — bt_do_embedding","text":"Initially function built upon sentence_transformers Python library, may expanded accept frameworks. feed documents list. can use hardware accelerators e.g. GPUs, speed computation. function currently returns object two additional attributes: embedding_model, n_documents, appended embeddings extraction later steps pipeline, e.g. merging data frames later important check many documents entered.","code":""},{"path":"/reference/bt_do_embedding.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Embed your documents — bt_do_embedding","text":"","code":"docs <- c(\"i am\", \"a list of\", \"documents\", \"to be embedded\")  embedder <- bt_make_embedder_st(\"aLL-minilm-l6-v2\")  embeddings <- bt_do_embedding(embedder, docs, accelerator = NULL) #>  #> Embedding proccess finished #> aLL-minilm-l6-v2 added to embeddings attributes"},{"path":"/reference/bt_do_reducing.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform dimensionality reduction on your embeddings — bt_do_reducing","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"Perform dimensionality reduction embeddings","code":""},{"path":"/reference/bt_do_reducing.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"","code":"bt_do_reducing(reducer, embeddings)"},{"path":"/reference/bt_do_reducing.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"reducer dimensionality reduction model embeddings embeddings","code":""},{"path":"/reference/bt_do_reducing.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"Embeddings reduced number dimensions","code":""},{"path":"/reference/bt_do_reducing.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform dimensionality reduction on your embeddings — bt_do_reducing","text":"","code":"# make reducuction model reducer <- bt_make_reducer_umap(n_neighbours = 2, n_components = 2)  # mock embeddings embeddings <- matrix(runif(50), nrow = 5, ncol = 10)  # reduce the embeddings reduced_embeddings <- bt_do_reducing(reducer, embeddings)"},{"path":"/reference/bt_empty_clusterer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","title":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","text":"Create empty clusterer skipping clustering step bertopic pipeline","code":""},{"path":"/reference/bt_empty_clusterer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","text":"","code":"bt_empty_clusterer()"},{"path":"/reference/bt_empty_clusterer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","text":"empty clustering model (Python class)","code":""},{"path":"/reference/bt_empty_clusterer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","text":"refers BaseCluster() function Python BERTopic library can used skip clustering step occurs part fitting bertopic model text data. Predicting topics achieved via clustering using function skip clustering step, pre-determined topics need passed bertopic model via y parameter bt_fit_model() function. can convenient cases already know topics documents belong. Creating bertopic model way allows utilise functionality bertopic model scenarios need employ topic discovery capabilities bertopic workflow.","code":""},{"path":"/reference/bt_empty_clusterer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an empty clusterer for skipping clustering step of bertopic pipeline — bt_empty_clusterer","text":"","code":"empty_clusterer <- bt_empty_clusterer()  clusterer <- bt_empty_clusterer()"},{"path":"/reference/bt_empty_embedder.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","title":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","text":"Create empty embedder skipping embedding step bertopic pipeline","code":""},{"path":"/reference/bt_empty_embedder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","text":"","code":"bt_empty_embedder()"},{"path":"/reference/bt_empty_embedder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","text":"empty embedding model (Python class)","code":""},{"path":"/reference/bt_empty_embedder.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","text":"refers BaseEmbedder() function Python BERTopic library can used skip document embedding step occurs part fitting bertopic model text data. convenient allows perform document embedding externally model gives full transparency terms embedding values can save time trialing fitting model various parameters.","code":""},{"path":"/reference/bt_empty_embedder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an empty embedder for skipping embedding step of bertopic pipeline — bt_empty_embedder","text":"","code":"empty_emebdder <- bt_empty_embedder()  embedder <- bt_empty_embedder()"},{"path":"/reference/bt_empty_reducer.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","title":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","text":"Create empty reducer skipping dimensionality reduction step bertopic pipeline","code":""},{"path":"/reference/bt_empty_reducer.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","text":"","code":"bt_empty_reducer()"},{"path":"/reference/bt_empty_reducer.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","text":"empty dimensionality reduction model (Python class)","code":""},{"path":"/reference/bt_empty_reducer.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","text":"refers BaseDimensionalityReduction() function Python BERTopic library can used skip dimensionality reduction step occurs part fitting bertopic model text data. convenient allows perform dimensionality reduction document embeddings externally model gives full transparency terms reduced embedding values can save time trialing fitting model various parameters.","code":""},{"path":"/reference/bt_empty_reducer.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an empty reducer for skipping dimensionality reduction step of bertopic pipeline — bt_empty_reducer","text":"","code":"empty_reducer <- bt_empty_reducer()  reducer <- bt_empty_reducer()"},{"path":"/reference/bt_fit_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a topic model on your documents & embeddings — bt_fit_model","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"already performed dimensionality reduction embeddings, can feed reduced dimension embeddings embeddings argument, make sure supply bt_compile_model base reducer (output bt_base_reducer()) NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. need specify output bt_fit_model function function changes input model place. decide explicitly assign function output, aware output model input model one another.","code":""},{"path":"/reference/bt_fit_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"","code":"bt_fit_model(model, documents, embeddings = NULL, topic_labels = NULL)"},{"path":"/reference/bt_fit_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"model Output bt_compile_model() another bertopic topic model documents documents topic model embeddings embeddings, can reduced dimensionality . embeddings provided, embedding model used reduction_model bt_compile_model used calculate reduce dimensionality embeddings. topic_labels Pre-existing labels, supervised topic modelling","code":""},{"path":"/reference/bt_fit_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"fitted BERTopic model","code":""},{"path":"/reference/bt_fit_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a topic model on your documents & embeddings — bt_fit_model","text":"","code":"if (FALSE) { # create the model with default parameters, then fit the model to the data model <- bt_compile_model() bt_fit_model(model = model, documents = docs)  # create the model with document embeddings already created and reduced # embeddings embedder <- bt_make_embedder_st(all-minilm-l6-v2) embeddings <- bt_do_embedding(embedder, docs)  # reduced embeddings reducer <- bt_make_reducer_umap() reduced_embeddings <-  bt_do_reducing(reducer, embeddings)  # model model <- bt_compile_model(embedding_model = bt_empty_embedder, reducer = bt_empty_reducer) bt_fit_model(model = model, documents = docs, embeddings = reduced_embeddings)  }"},{"path":"/reference/bt_make_clusterer_agglomerative.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"Create Agglomerative Clustering clustering model","code":""},{"path":"/reference/bt_make_clusterer_agglomerative.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"","code":"bt_make_clusterer_agglomerative(..., n_clusters = 20L)"},{"path":"/reference/bt_make_clusterer_agglomerative.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"... Additional arguments sent sklearn.cluster.AgglomerativeClustering() n_clusters number clusters search (enter integer typing L number)","code":""},{"path":"/reference/bt_make_clusterer_agglomerative.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"Agglomerative Clustering clustering model (Python object)","code":""},{"path":"/reference/bt_make_clusterer_agglomerative.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an Agglomerative Clustering clustering model — bt_make_clusterer_agglomerative","text":"","code":"clustering_model <- bt_make_clusterer_agglomerative( n_clusters = 15L)  agglomerative_model <- bt_make_clusterer_agglomerative(n_clusters = 10L)"},{"path":"/reference/bt_make_clusterer_hdbscan.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"Instantiates HDBSCAN clustering model using hdbscan Python library.","code":""},{"path":"/reference/bt_make_clusterer_hdbscan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"","code":"bt_make_clusterer_hdbscan(   ...,   min_cluster_size = 10L,   min_samples = 10L,   metric = \"euclidean\",   cluster_selection_method = c(\"eom\", \"leaf\"),   prediction_data = FALSE )"},{"path":"/reference/bt_make_clusterer_hdbscan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"... Additional arguments sent hdbscan.HDBSCAN() min_cluster_size Minimum number data points cluster, enter integer adding L number min_samples Controls number outliers generated, lower value = fewer outliers. metric Distance metric calculate clusters cluster_selection_method method used select clusters. Default \"eom\". prediction_data Set TRUE intend using model functions hdbscan.prediction eg. using bt_outliers_probabilities","code":""},{"path":"/reference/bt_make_clusterer_hdbscan.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"instance HDBSCAN clustering model (Python object.","code":""},{"path":[]},{"path":"/reference/bt_make_clusterer_hdbscan.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an HDBSCAN clustering model — bt_make_clusterer_hdbscan","text":"","code":"# using minkowski metric for calculating distance between documents - when using minkowski metric, a value for p must be specified as an additional argument clustering_model <- bt_make_clusterer_hdbscan(metric = \"minkowski\", p = 1.5)  # specify integer numeric inputs as integer, using additional gen_min_span_tree argument clusterer = bt_make_clusterer_hdbscan(min_cluster_size = 5L, gen_min_span_tree = TRUE)  # not specifying numeric inputs as integers (converted to integers internally) clusterer = bt_make_clusterer_hdbscan(min_cluster_size = 5, cluster_selection_method = \"leaf\")"},{"path":"/reference/bt_make_clusterer_kmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a kmeans clustering model — bt_make_clusterer_kmeans","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"Create kmeans clustering model","code":""},{"path":"/reference/bt_make_clusterer_kmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"","code":"bt_make_clusterer_kmeans(..., n_clusters = 10L)"},{"path":"/reference/bt_make_clusterer_kmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"... Additional arguments sent sklearn.cluster.KMeans() n_clusters number clusters search (enter integer typing L number)","code":""},{"path":"/reference/bt_make_clusterer_kmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"kMeans clustering model (Python object)","code":""},{"path":"/reference/bt_make_clusterer_kmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a kmeans clustering model — bt_make_clusterer_kmeans","text":"","code":"# models with different values for number of clusters clustering_model <- bt_make_clusterer_kmeans(n_clusters = 15L) clustering_model <- bt_make_clusterer_kmeans(n_clusters = 10L)  # specifying additional arguments clustering_model <- bt_make_clusterer_kmeans(n_clusters = 10L, verbose = TRUE)"},{"path":"/reference/bt_make_ctfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"function creates instance ClassTfidfTransformer bertopic.vectorizers module, provided arguments. used generate representations topics selecting words frequent within topic less frequent entire corpus.","code":""},{"path":"/reference/bt_make_ctfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"","code":"bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)"},{"path":"/reference/bt_make_ctfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"reduce_frequent_words frequent words reduced? Default TRUE. bm25_weighting BM25 weighting used? Default FALSE.","code":""},{"path":"/reference/bt_make_ctfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"ctfidf model (Python object).","code":""},{"path":"/reference/bt_make_ctfidf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an instance of the ClassTfidfTransformer from the bertopic.vectorizers module — bt_make_ctfidf","text":"","code":"ctfidf <- bt_make_ctfidf(reduce_frequent_words = TRUE, bm25_weighting = FALSE)"},{"path":"/reference/bt_make_embedder_flair.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","title":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","text":"Create embedding model using model available Flair Library","code":""},{"path":"/reference/bt_make_embedder_flair.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","text":"","code":"bt_make_embedder_flair(   model,   ...,   flair_class = c(\"FlairEmbeddings\", \"TransformerWordEmbeddings\",     \"TransformerDocumentEmbeddings\", \"WordEmbeddings\") )"},{"path":"/reference/bt_make_embedder_flair.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","text":"model name model used create embeddings ... Additional arguments passed selected Flair class flair_class BertopicR currently compatible 4 Flair embedding classes: FlairEmbeddings, WordEmbeddings, TransformerWordEmbeddings TransformerDocumentEmbeddings. chose perform word embeddings rather document embeddings, bt_do_embedding pool word embeddings document calculate mean value.","code":""},{"path":"/reference/bt_make_embedder_flair.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","text":"embedding model, formed according model defined, can input be_do_embedding create document embeddings","code":""},{"path":"/reference/bt_make_embedder_flair.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an embedding model using a model available from the Flair Library — bt_make_embedder_flair","text":"","code":"# \\donttest{ # Flair Embedding, reducing chars_per_chunk to help with memory issues embedder <- bt_make_embedder_flair(model = \"news-forward\", flair_class = \"FlairEmbeddings\", chars_per_chunk = 400L) #> Error in bt_make_embedder_flair(model = \"news-forward\", flair_class = \"FlairEmbeddings\",     chars_per_chunk = 400L): flair is not in installed packages of current environment, run reticulate::py_install(\"flair\"). #>  #>             Note that if you receive a module not found error, you may need to instead run reticulate::py_install(\"flair\", pip = TRUE) to force installation with pip instead of conda.  # Transformer Document Embedding embedder <- bt_make_embedder_flair(model = \"roberta-base\", flair_class = \"TransformerDocumentEmbeddings\") #> Error in bt_make_embedder_flair(model = \"roberta-base\", flair_class = \"TransformerDocumentEmbeddings\"): flair is not in installed packages of current environment, run reticulate::py_install(\"flair\"). #>  #>             Note that if you receive a module not found error, you may need to instead run reticulate::py_install(\"flair\", pip = TRUE) to force installation with pip instead of conda. # }"},{"path":"/reference/bt_make_embedder_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","title":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","text":"Create embedding model using model available OpenAI Library","code":""},{"path":"/reference/bt_make_embedder_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","text":"","code":"bt_make_embedder_openai(   model = \"text-embedding-ada-002\",   openai_api_key = \"sk-\" )"},{"path":"/reference/bt_make_embedder_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","text":"model name openai model used create embeddings openai_api_key OpenAI API key required use OpenAI API can found OpenAI website","code":""},{"path":"/reference/bt_make_embedder_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","text":"embedding model, formed according model defined, can input be_do_embedding create document embeddings","code":""},{"path":"/reference/bt_make_embedder_openai.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an embedding model using a model available from the OpenAI Library — bt_make_embedder_openai","text":"","code":"if (FALSE) { embedder <- bt_make_embedder_openai(model = \"text-embedding-ada-002\", openai_api_key = \"sk-\") }"},{"path":"/reference/bt_make_embedder_spacy.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","title":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","text":"Create embedding model using model available Spacy Library","code":""},{"path":"/reference/bt_make_embedder_spacy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","text":"","code":"bt_make_embedder_spacy(model, ..., prefer_gpu = TRUE, exclude = NULL)"},{"path":"/reference/bt_make_embedder_spacy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","text":"model pipeline used make predictions ... additional arguments sent spacy.load function prefer_gpu TRUE use gpu available exclude name pipeline components exclude","code":""},{"path":"/reference/bt_make_embedder_spacy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","text":"embedding model, formed according model defined, can input be_do_embedding create document embeddings","code":""},{"path":"/reference/bt_make_embedder_spacy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an embedding model using a model available from the Spacy Library — bt_make_embedder_spacy","text":"","code":"# \\donttest{ # specify a non-transformer model, excluding features not required embedder <- bt_make_embedder_spacy(model = \"en_core_web_md\", exclude = c(\"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\")) #> spacy is not in installed packages of current environment, run reticulate::py_install(\"spacy\"). #> Error in py_module_import(module, convert = convert): ModuleNotFoundError: No module named 'spacy' #> Run `reticulate::py_last_error()` for details.  # specify a transformer model and exclude features not required embedder <- bt_make_embedder_spacy(model = \"en_core_web_trf\", exclude = c(\"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\")) #> spacy is not in installed packages of current environment, run reticulate::py_install(\"spacy\"). #> Error in py_module_import(module, convert = convert): ModuleNotFoundError: No module named 'spacy' #> Run `reticulate::py_last_error()` for details. # }"},{"path":"/reference/bt_make_embedder_st.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an embedding model using sentence-transformers — bt_make_embedder_st","title":"Create an embedding model using sentence-transformers — bt_make_embedder_st","text":"Initially function built upon sentence_transformers Python library, may expanded accept frameworks. feed documents list. can use hardware accelerators e.g. GPUs, speed computation.","code":""},{"path":"/reference/bt_make_embedder_st.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an embedding model using sentence-transformers — bt_make_embedder_st","text":"","code":"bt_make_embedder_st(model)"},{"path":"/reference/bt_make_embedder_st.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an embedding model using sentence-transformers — bt_make_embedder_st","text":"model Name embedding model string (case sensitive)","code":""},{"path":"/reference/bt_make_embedder_st.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an embedding model using sentence-transformers — bt_make_embedder_st","text":"embedding model, formed according model defined, can input be_do_embedding create document embeddings","code":""},{"path":"/reference/bt_make_embedder_st.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an embedding model using sentence-transformers — bt_make_embedder_st","text":"","code":"embedder <- bt_make_embedder_st(\"all-mpnet-base-v2\")  embedder <- bt_make_embedder_st(\"aLL-minilm-l6-v2\")"},{"path":"/reference/bt_make_reducer_pca.html","id":null,"dir":"Reference","previous_headings":"","what":"Create pca dimensionality reduction model — bt_make_reducer_pca","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"function wraps PCA functionality Python's sklearn package use R via reticulate. allows perform dimension reduction high-dimensional data, intended use BertopicR pipeline. concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_empty_reducer().","code":""},{"path":"/reference/bt_make_reducer_pca.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"","code":"bt_make_reducer_pca(   n_components,   ...,   svd_solver = c(\"auto\", \"full\", \"arpack\", \"randomized\") )"},{"path":"/reference/bt_make_reducer_pca.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"n_components Number components keep ... Sent sklearn.decomposition.PCA function adding additional arguments svd_solver method reducing components can auto, full, arpack, randomized","code":""},{"path":"/reference/bt_make_reducer_pca.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"PCA Model can input bt_do_reducing reduce dimensions data","code":""},{"path":[]},{"path":"/reference/bt_make_reducer_pca.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create pca dimensionality reduction model — bt_make_reducer_pca","text":"","code":"# using default svd_solver reducer <- bt_make_reducer_pca(n_components = 100)  # speciying extra pca arguments reducer <- bt_make_reducer_pca (n_components = 20, svd_solver = \"full\", random_state = 42L)"},{"path":"/reference/bt_make_reducer_truncated_svd.html","id":null,"dir":"Reference","previous_headings":"","what":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"function wraps Truncated SVD (Single Value Decomposition) functionality Python's sklearn package use R via reticulate. allows perform dimension reduction high-dimensional data. intended use BertopicR pipeline. concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_empty_reducer().","code":""},{"path":"/reference/bt_make_reducer_truncated_svd.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"","code":"bt_make_reducer_truncated_svd(   n_components,   ...,   n_iter = 5L,   svd_solver = c(\"randomized\", \"arpack\") )"},{"path":"/reference/bt_make_reducer_truncated_svd.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"n_components Number components keep ... Sent sklearn.decomposition Truncated SVD function adding additional arguments n_iter Number iterations randomised svd solver. used svd solver \"arpack\". svd_solver method reducing components can arpack randomized","code":""},{"path":"/reference/bt_make_reducer_truncated_svd.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"Truncated SVD Model can input bt_do_reducing reduce dimensions data","code":""},{"path":[]},{"path":"/reference/bt_make_reducer_truncated_svd.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Created Truncated SVD dimensionality reduction model — bt_make_reducer_truncated_svd","text":"","code":"reducer <- bt_make_reducer_truncated_svd(n_components = 5)"},{"path":"/reference/bt_make_reducer_umap.html","id":null,"dir":"Reference","previous_headings":"","what":"Create umap dimensionality reduction model — bt_make_reducer_umap","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"function wraps UMAP functionality Python's umap-learn package use R via reticulate. allows perform dimension reduction high-dimensional data, intended use BertopicR pipeline/","code":""},{"path":"/reference/bt_make_reducer_umap.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"","code":"bt_make_reducer_umap(   ...,   n_neighbours = 15L,   n_components = 5L,   min_dist = 0,   metric = \"euclidean\",   random_state = 42L,   low_memory = FALSE,   verbose = TRUE )"},{"path":"/reference/bt_make_reducer_umap.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"... Sent umap.UMAP python function adding additional arguments n_neighbours size local neighbourhood (terms number neighboring data points) used manifold approximation (default: 15). n_components number dimensions reduce (default: 5). min_dist minimum distance points low-dimensional representation (default: 0.0). metric metric use distance computation (default: \"euclidean\"). random_state seed used random number generator (default: 42). low_memory Loogical, use low memory version UMAP (default: FALSE) verbose Logical flag indicating whether report progress dimension reduction (default: TRUE).","code":""},{"path":"/reference/bt_make_reducer_umap.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"UMAP Model can input bt_do_reducing reduce dimensions data","code":""},{"path":"/reference/bt_make_reducer_umap.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"concerned processing time, likely want reduce dimensions dataset . case, compiling model bt_compile_model call reducer <- bt_empty_reducer(). low_memory = TRUE currently inadvisable trial error suggests results robust later clustering.","code":""},{"path":[]},{"path":"/reference/bt_make_reducer_umap.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create umap dimensionality reduction model — bt_make_reducer_umap","text":"","code":"# using euclidean distance measure and specifying numeric inputs as integers reducer <- bt_make_reducer_umap(n_neighbours = 15L, n_components = 10L, metric = \"euclidean\")  # using euclidean distance measure and not specifying numeric inputs as integers (done internally in function) reducer <- bt_make_reducer_umap(n_neighbours = 15, n_components = 10, metric = \"euclidean\")   # using cosine distance measure and not specifying numeric inputs as integers (done internally in function) reducer <- bt_make_reducer_umap(n_neighbours = 20, n_components = 6, metric = \"cosine\")"},{"path":"/reference/bt_make_vectoriser.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a text vectoriser — bt_make_vectoriser","title":"Create a text vectoriser — bt_make_vectoriser","text":"function uses Python's sklearn feature extraction count vectorisation. creates CountVectorizer object specified parameters. CountVectorizer way convert text data vectors model input. Used inside BertopicR topic modelling pipeline.","code":""},{"path":"/reference/bt_make_vectoriser.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a text vectoriser — bt_make_vectoriser","text":"","code":"bt_make_vectoriser(   ...,   ngram_range = c(1L, 2L),   stop_words = \"english\",   min_frequency = 0.1,   max_features = NULL )"},{"path":"/reference/bt_make_vectoriser.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a text vectoriser — bt_make_vectoriser","text":"... Additional parameters passed sklearn's CountVectorizer ngram_range vector length 2 (default c(1, 2)) indicating lower upper boundary range n-values different word n-grams char n-grams extracted features. values n min_n <= n <= max_n used. example ngram_range c(1, 1) means unigrams, c(1, 2) means unigrams bigrams, c(2, 2) means bigrams. stop_words String (default 'english'). string, passed _check_stop_list appropriate stop list returned. 'english' currently default. min_frequency Integer float (default 0.1). building vocabulary ignore terms corpus frequency strictly lower given threshold. min_frequency explicitly defined integer, assumed represent absolute count. min_frequency explicitly specified integer 0 1, assumed represent proportion documents, whole number assumed represent absolute count. max_features Integer NULL (default NULL). NULL, build vocabulary considers top max_features ordered term frequency across corpus.","code":""},{"path":"/reference/bt_make_vectoriser.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a text vectoriser — bt_make_vectoriser","text":"sklearn CountVectorizer object configured provided parameters","code":""},{"path":"/reference/bt_make_vectoriser.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a text vectoriser — bt_make_vectoriser","text":"","code":"# vectoriser model that converts text docs to ngrams with between 1 - 2 tokens vectoriser <- bt_make_vectoriser(ngram_range = c(1, 2), stop_words = \"english\")  # vectoriser model that converts text docs to ngrams with between 1 - 3 tokens vectoriser <- bt_make_vectoriser(ngram_range = c(1, 3), stop_words = \"english\")  # You can implement custom stopwords or stopwords from other sources if (FALSE) { stopwords_cat <- tm::stopwords(kind = \"catalan\") vectoriser <- bt_make_vectoriser(ngram_range = c(1, 3), stop_words = stopwords_cat) }  custom_stopwords <- c(\"these\", \"words\", \"are\", \"not\", \"helpful\") vectoriser <- bt_make_vectoriser(ngram_range = c(1,2), stop_words = custom_stopwords)"},{"path":"/reference/bt_merge_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Merges list(s) of topics together — bt_merge_topics","title":"Merges list(s) of topics together — bt_merge_topics","text":"Merge topics already-fitted BertopicR model. can feed list, merge topics together, list lists perform merges multiple groups topics. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. need specify output bt_fit_model function function changes input model place. decide explicitly assign function output, aware output model input model one another.","code":""},{"path":"/reference/bt_merge_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merges list(s) of topics together — bt_merge_topics","text":"","code":"bt_merge_topics(fitted_model, documents, topics_to_merge)"},{"path":"/reference/bt_merge_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merges list(s) of topics together — bt_merge_topics","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fitted topics_to_merge list (list lists/vectors) topics created bertopic model wish merge. Topics given numeric form.","code":""},{"path":"/reference/bt_merge_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merges list(s) of topics together — bt_merge_topics","text":"bertopic model specified topics merged","code":""},{"path":"/reference/bt_merge_topics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Merges list(s) of topics together — bt_merge_topics","text":"function updates model topics topics_to_merge list become 1 topic. grouped topics take topic representation (Name) first topic list. number topics can merged together, like merge two separate groups topics, must pass list lists/vectors topics_to_merge eg. list(c(1L, 3L), c(0L, 6L, 7L)) list(list(1L, 3L), list(0L, 6L, 7L))","code":""},{"path":"/reference/bt_merge_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merges list(s) of topics together — bt_merge_topics","text":"","code":"if (FALSE) {  # merge two topics bt_merge_topics(fitted_model = model, documents = documents, topics_to_merge = list(1L, 2L))  # merge three topics bt_merge_topics(fitted_model = model, documents = documents, topics_to_merge = list(1L, 2L, 3L))  # merge multiple sets of topics as a list of vectors bt_merge_topics(fitted_model = model, documents = documents, topics_to_merge = list(c(1L, 3L), c(0L, 6L, 7L), c(2L, 12L)))  # merge multiple sets of topics as a list of lists bt_merge_topics(fitted_model = model, documents = documents, topics_to_merge = list(list(1L, 3L), list(0L, 6L, 7L), list(2L, 12L))) }"},{"path":"/reference/bt_outliers_ctfidf.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"Calculates cosine similarity c-TF-IDF documents topics redistributes outliers based topic highest similarity . Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"/reference/bt_outliers_ctfidf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"","code":"bt_outliers_ctfidf(fitted_model, documents, topics, threshold = 0.3)"},{"path":"/reference/bt_outliers_ctfidf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents threshold minimum probability outlier reassigned","code":""},{"path":"/reference/bt_outliers_ctfidf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"df document, old topic, new topic","code":""},{"path":"/reference/bt_outliers_ctfidf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"possible chain outlier reduction methods together operation works list topics input argument, can vary. see examples able perform one outlier reduction method, eg. bt_outliers_embeddings, output list potential new topics, input list another outlier reduction method, eg. bt_outliers_ctfidf, determine output topic suggestions based input list. way can use aspects multiple outlier reduction strategies chain together.","code":""},{"path":"/reference/bt_outliers_ctfidf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistributes outliers using c-TF-IDF scores — bt_outliers_ctfidf","text":"","code":"if (FALSE) { # Reducing outliers original clustering model identified outliers <- bt_outliers_ctfidf(fitted_model = topic_model, documents = docs, topics = topic_model$topics_)  # Using chain strategies to build on outliers identified by another reduction strategy to redistribute outlier docs # using embeddings to redistribute outliers outliers_embed <- bt_outliers_embedings(fitted_model = topic_model, documents = docs, topics = topic_model$topics_)  # using ctfidf outlier reduction method on top of embeddings method to redistribute outliers outliers_chain <- bt_outliers_ctfidf(fitted_model = topic_model, documents = docs, topics = outliers_embed$new_topics, threshold = 0.2)  }"},{"path":"/reference/bt_outliers_embeddings.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using embeddings — bt_outliers_embeddings","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"Uses cosine similarity document embeddings find topic closest outlier document reassigns documents accordingly. Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"/reference/bt_outliers_embeddings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"","code":"bt_outliers_embeddings(   fitted_model,   documents,   topics,   embeddings,   embedding_model = NULL,   threshold = 0.3 )"},{"path":"/reference/bt_outliers_embeddings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents embeddings embeddings used create topics. embedding_model instantiate model embedding model need pass one threshold minimum probability outlier reassigned","code":""},{"path":"/reference/bt_outliers_embeddings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"df document, old topic, new topic","code":""},{"path":"/reference/bt_outliers_embeddings.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"possible chain outlier reduction methods together operation works list topics input argument, can vary. see examples able perform one outlier reduction method, eg. bt_outliers_tokenset_similarity, output list potential new topics, input list another outlier reduction method, eg. bt_outliers_embeddings, determine output topic suggestions based input list. way can use aspects multiple outlier reduction strategies chain together.","code":""},{"path":"/reference/bt_outliers_embeddings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistributes outliers using embeddings — bt_outliers_embeddings","text":"","code":"if (FALSE) { # Reducing outliers original clustering model identified outliers <- bt_outliers_embeddings(fitted_model = topic_model, documents = docs, topics = topic_model$topics_, embeddings = embeddings)  # Using chain strategies to build on outliers identified by another reduction strategy to redistribute outlier docs # using tokenset similarity to redistribute outliers outliers_ts <- bt_outliers_tokenset_similarity(fitted_model = topic_model, documents = docs, topics = topic_model$topics_)  # using embedding outlier reduction method on top of tokenset similarity method to redistribute outliers outliers_chain <- bt_outliers_embeddings(fitted_model = topic_model, documents = docs, topics = outliers_ts$new_topics, embeddings = embeddings)  }"},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":null,"dir":"Reference","previous_headings":"","what":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"Divides documents tokensets calculates c-TF-IDF similarity tokenset topic. outlier document, similarity scores tokenset topic summed together topic outlier redistributed topic highest similarity. Note purpose function obtain new list topics can used update model, make changes model , topic classification model outputs change running function. bt_update_topics function needs used make change model .","code":""},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"","code":"bt_outliers_tokenset_similarity(   fitted_model,   documents,   topics,   ...,   window = 4L,   stride = 1L,   threshold = 0.3 )"},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit topics current topics associated documents ... Optional additional parameters passed approximate_distribution function, e.g. batch_size window size moving window number tokens tokenset stride far window move step (number words skip moving next tokenset) threshold minimum probability outlier reassigned","code":""},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"df document, old topic, new topic","code":""},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"possible chain outlier reduction methods together operation works list topics input argument, can vary. see examples able perform one outlier reduction method, eg. bt_outliers_embeddings, output list potential new topics, input list another outlier reduction method, eg. bt_outliers_tokenset_similarity, determine output topic suggestions based input list. way can use aspects multiple outlier reduction strategies chain together.","code":""},{"path":"/reference/bt_outliers_tokenset_similarity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Redistributes outliers using tokenset c-TF-IDF scores — bt_outliers_tokenset_similarity","text":"","code":"if (FALSE) { # Reducing outliers original clustering model identified outliers <- bt_outliers_tokenset_similarity(fitted_model = topic_model, documents = docs, topics = topic_model$topics_)  # Using chain strategies to build on outliers identified by another reduction strategy to redistribute outlier docs # using embeddings to redistribute outliers outliers_embed <- bt_outliers_embedings(fitted_model = topic_model, documents = docs, topics = topic_model$topics_, embeddings = embeddings, threshold = 0.5)  # using tokenset similarity outlier reduction method on top of embeddings method to redistribute outliers outliers_chain <- bt_outliers_tokenset_similarity(fitted_model = topic_model, documents = docs, topics = outliers_embed$new_topics, threshold = 0.2)  }"},{"path":"/reference/bt_representation_hf.html","id":null,"dir":"Reference","previous_headings":"","what":"Use Huggingface models to create topic representation — bt_representation_hf","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"Use Huggingface models create topic representation","code":""},{"path":"/reference/bt_representation_hf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"","code":"bt_representation_hf( fitted_model, documents, task, hf_model, ..., default_prompt = \"keywords\", nr_samples = 500, nr_repr_docs = 20, diversity = 10, custom_prompt = NULL )"},{"path":"/reference/bt_representation_hf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"fitted_model fitted bertopic model documents documents topic model fitted task Task defining pipeline returned. See https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html information. Use \"text-generation\" gpt-like models \"text2text-generation\" T5-like models hf_model model used pipeline make predictions ... arguments sent transformers.pipeline function default_prompt Whether use \"keywords\" \"documents\" default prompt. Passing custom_prompt render argument NULL. Default \"keywords\" prompt. nr_samples Number sample documents representative docs chosen nr_repr_docs Number representative documents sent huggingface model diversity diversity documents sent huggingface model. 0 = diversity, 1 = max diversity. custom_prompt custom prompt used pipeline. specified, \"keywords\" \"documents\" default_prompt used. Use \"[KEYWORDS]\" \"[DOCUMENTS]\" prompt decide keywords documents inserted.","code":""},{"path":"/reference/bt_representation_hf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"updated representation topic","code":""},{"path":"/reference/bt_representation_hf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Use Huggingface models to create topic representation — bt_representation_hf","text":"Representative documents chosen topic sampling (nr_samples) number documents topic calculating documents representative topic c-tf-idf cosine similarity topic individual documents. representative documents (number defined nr_repr_docs parameter) extracted passed huggingface model topic description predicted.","code":""},{"path":"/reference/bt_representation_keybert.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model using keybert — bt_representation_keybert","title":"Create representation model using keybert — bt_representation_keybert","text":"creates topic representations based KeyBERT algorithm.","code":""},{"path":"/reference/bt_representation_keybert.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model using keybert — bt_representation_keybert","text":"","code":"bt_representation_keybert( fitted_model,  documents,  document_embeddings, embedding_model, top_n_words = 10, nr_repr_docs = 50, nr_samples = 500, nr_candidate_words = 100)"},{"path":"/reference/bt_representation_keybert.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model using keybert — bt_representation_keybert","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents fitted_model fitted document_embeddings embeddings used fit model, dimensions specified embedder pass embedding model embedding_model model used create embeddings passed. used create word embeddings compared topic embeddings using cosine similarity. top_n_words number keywords/phrases extracted nr_repr_docs number documents used create topic embeddings nr_samples number samples select representative docs topic nr_candidate_words number words examine per topic","code":""},{"path":"/reference/bt_representation_keybert.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model using keybert — bt_representation_keybert","text":"KeyBERTInspired representation model","code":""},{"path":"/reference/bt_representation_keybert.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create representation model using keybert — bt_representation_keybert","text":"KeyBERT python package used extraction key words documents. works : Selecting representative documents (nr_repr_docs) topic based c_tf_idf cosine similarity documents topics. achieved sampling nr_samples documents topic calculating c_tf_idf score choosing top nr_repr_docs . Candidate words (nr_candidate_words) selected topic based c_tf_idf scores topic Topic embeddings created averaging embeddings representative documents topic compared, using cosine similarity, candidate word embeddings give similarity score word topic top_n_words highest cosine similarity topic used represent topic","code":""},{"path":"/reference/bt_representation_mmr.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"Calculates maximal marginal relevance candidate words documents. Considers similarity keywords phrases already selected keywords phrases chooses representation based maximise diversity.","code":""},{"path":"/reference/bt_representation_mmr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"","code":"bt_representation_mmr( fitted_model, embedding_model, diversity = 0.1, top_n_words = 10)"},{"path":"/reference/bt_representation_mmr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. embedding_model embedding model used embed keywords selected potential representative words. compatible sentence transformer models point. diversity diverse representation words/phrases . 0 = diverse, 1 = completely diverse top_n_words Number keywords/phrases extracted","code":""},{"path":"/reference/bt_representation_mmr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model using Maximal Marginal Relevance — bt_representation_mmr","text":"MaximalMarginalRelevance representation model","code":""},{"path":"/reference/bt_representation_openai.html","id":null,"dir":"Reference","previous_headings":"","what":"Create representation model that uses OpenAI text generation models — bt_representation_openai","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"Representative documents chosen topic sampling (nr_samples) number documents topic calculating documents representative topic c-tf-idf cosine similarity topic individual documents. representative documents (number defined nr_repr_docs parameter) extracted passed  OpenAI API generate topic labels based one Completion (chat = FALSE) ChatCompletion (chat = TRUE) models.","code":""},{"path":"/reference/bt_representation_openai.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"","code":"bt_representation_openai( fitted_model, documents, openai_model = \"text-ada-001\", nr_repr_docs = 10, nr_samples = 500, chat = FALSE, api_key = \"sk-\", delay_in_seconds = NULL, prompt = NULL, diversity = NULL)"},{"path":"/reference/bt_representation_openai.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents used fit fitted_model openai_model openai model use. using gpt-3.5 model, set chat = TRUE nr_repr_docs number representative documents per topic send openai model nr_samples Number sample documents representative docs chosen chat set TRUE using gpt-3.5 model api_key OpenAI API key required use OpenAI API can found OpenAI website delay_in_seconds delay seconds consecutive prompts, avoid rate limit errors. prompt prompt used openai model. NULL, default prompt used. diversity diversity documents sent huggingface model. 0 = diversity, 1 = max diversity.","code":""},{"path":"/reference/bt_representation_openai.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create representation model that uses OpenAI text generation models — bt_representation_openai","text":"OpenAI representation model","code":""},{"path":"/reference/bt_update_topics.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Topic Representations — bt_update_topics","title":"Update Topic Representations — bt_update_topics","text":"Updates topics representations based document-topic classification described list new_topics. initiating model bt_compile_model, want manipulate topic representations must use vectoriser/ctfidf model, can used bt_compile_model. NOTE: bertopic model working pointer python object point memory. means input output model differentiated without explicitly saving model performing operation. need specify output bt_fit_model function function changes input model place. decide explicitly assign function output, aware output model input model one another.","code":""},{"path":"/reference/bt_update_topics.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Topic Representations — bt_update_topics","text":"","code":"bt_update_topics(   fitted_model,   documents,   new_topics = NULL,   representation_model = NULL,   vectoriser_model = NULL,   ctfidf_model = NULL )"},{"path":"/reference/bt_update_topics.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Topic Representations — bt_update_topics","text":"fitted_model Output bt_fit_model() another bertopic topic model. model must fitted data. documents documents model fit new_topics Topics update model representation_model model updating topic representations vectoriser_model Model vectorising input topic representations (Python object) ctfidf_model Model performing class-based tf-idf (ctf-idf) (Python object)","code":""},{"path":"/reference/bt_update_topics.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Topic Representations — bt_update_topics","text":"updated model","code":""},{"path":"/reference/bt_update_topics.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update Topic Representations — bt_update_topics","text":"NOTE: using function update outlier topics, may lead errors topic reduction topic merging techniques used afterwards. reason assign -1 document topic 1 another -1 document topic 2, unclear map -1 documents. matched topic 1 2.","code":""},{"path":"/reference/bt_update_topics.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update Topic Representations — bt_update_topics","text":"","code":"if (FALSE) { # update model with new topic distribution # reduce outliers outliers <- bt_outliers_ctfidf(fitted_model = topic_model, documents = docs, threshold = 0.2)  # update the model with the new topic distribution bt_update_topics(fitted_model = topic_model, documents = docs, new_topics = outliers$new_topics)  # update topic representation bt_update_topics(fitted_model = topic_model, documents = docs, vectoriser_model = update_vec_model) }"},{"path":"/reference/check_python_dependencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Check that dependencies are loaded — check_python_dependencies","title":"Check that dependencies are loaded — check_python_dependencies","text":"Check dependencies loaded","code":""},{"path":"/reference/check_python_dependencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check that dependencies are loaded — check_python_dependencies","text":"","code":"check_python_dependencies()"},{"path":"/reference/check_python_dependencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check that dependencies are loaded — check_python_dependencies","text":"message confirming whether dependencies loaded","code":""},{"path":"/reference/install_python_dependencies.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Python Dependencies — install_python_dependencies","title":"Install Python Dependencies — install_python_dependencies","text":"Install Python Dependencies","code":""},{"path":"/reference/install_python_dependencies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Python Dependencies — install_python_dependencies","text":"","code":"install_python_dependencies()"},{"path":"/reference/install_python_dependencies.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Install Python Dependencies — install_python_dependencies","text":"Nothing","code":""},{"path":"/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling rhs(lhs).","code":""}]
